{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a923da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import joblib\n",
    "from src.data_loader import get_all_data, split_date_lazy\n",
    "from src.feature_engineering import build_features_from_purchases, build_labels\n",
    "from src.trainer import train_model\n",
    "from src import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ecbfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading and splitting data...\n",
      "ğŸ“‚ Äang quÃ©t dá»¯ liá»‡u tá»« thÆ° má»¥c: c:\\Users\\HP\\Desktop\\my_rec_system\\data\n",
      "âœ… ÄÃ£ load Items\n",
      "âœ… ÄÃ£ load Users\n",
      "âœ… ÄÃ£ load Purchases\n",
      "âœ… Data split completed.\n",
      "   - Hist samples: 29,580,965\n",
      "   - Recent samples: 3,099,667\n",
      "   - Val samples: 3,049,193\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 2. LOAD & SPLIT DATA USING CUSTOM UTILITY\n",
    "# ======================================================================\n",
    "print(\"ğŸ“‚ Loading and splitting data...\")\n",
    "\n",
    "# 1. Load dá»¯ liá»‡u\n",
    "lf_items, lf_users, lf_purchases = get_all_data(\"data\")\n",
    "\n",
    "# 2. Sá»­ dá»¥ng hÃ m cá»§a báº¡n Ä‘á»ƒ chia táº­p\n",
    "# HÃ m nÃ y tá»± Ä‘á»™ng xá»­ lÃ½ cast datetime náº¿u created_date Ä‘ang lÃ  String\n",
    "lf_purchase_hist, lf_purchase_recent, lf_purchase_val = split_date_lazy(\n",
    "    lf_purchases, \n",
    "    date_column_name=\"created_date\"\n",
    ")\n",
    "\n",
    "# 3. Kiá»ƒm tra nhanh káº¿t quáº£ báº±ng collect_schema (khÃ´ng tá»‘n RAM)\n",
    "print(\"âœ… Data split completed.\")\n",
    "print(f\"   - Hist samples: {lf_purchase_hist.select(pl.len()).collect().item():,}\")\n",
    "print(f\"   - Recent samples: {lf_purchase_recent.select(pl.len()).collect().item():,}\")\n",
    "print(f\"   - Val samples: {lf_purchase_val.select(pl.len()).collect().item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08457b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sá»‘ user trong Hist: 2,170,516\n",
      "Sá»‘ user trong Recent: 650,383\n",
      "Sá»‘ user giao thoa: 507,012\n"
     ]
    }
   ],
   "source": [
    "# Kiá»ƒm tra xem cÃ³ bao nhiÃªu user xuáº¥t hiá»‡n á»Ÿ cáº£ Hist vÃ  Recent\n",
    "user_hist = lf_purchase_hist.select(\"customer_id\").unique().collect()\n",
    "user_recent = lf_purchase_recent.select(\"customer_id\").unique().collect()\n",
    "\n",
    "intersection = user_hist.join(user_recent, on=\"customer_id\", how=\"inner\")\n",
    "print(f\"Sá»‘ user trong Hist: {user_hist.height:,}\")\n",
    "print(f\"Sá»‘ user trong Recent: {user_recent.height:,}\")\n",
    "print(f\"Sá»‘ user giao thoa: {intersection.height:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8961326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Äang chuáº©n bá»‹ dá»¯ liá»‡u ná»n táº£ng cho Model vÃ  Inference...\n",
      " âœ… ÄÃ£ náº¡p Ground Truth: 391,900 users.\n",
      " ğŸ› ï¸ Äang tÃ­nh toÃ¡n Ma tráº­n Co-occurrence...\n",
      " ğŸ› ï¸ Äang xÃ¢y dá»±ng infer_features_df tá»« full_history...\n",
      "\n",
      "ğŸš€ KHá»I Táº O XONG! CÃ¡c biáº¿n Ä‘Ã£ khá»›p Schema String/Int32.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 3. INITIALIZE CORE DATA & CO-OCCURRENCE (FIXED FOR SCHEMA)\n",
    "# ======================================================================\n",
    "import os\n",
    "import joblib\n",
    "import polars as pl\n",
    "\n",
    "print(\"ğŸ—ï¸ Äang chuáº©n bá»‹ dá»¯ liá»‡u ná»n táº£ng cho Model vÃ  Inference...\")\n",
    "\n",
    "# 1. Gá»™p toÃ n bá»™ lá»‹ch sá»­ mua hÃ ng \n",
    "# LÆ¯U Ã: Pháº£i gá»™p cáº£ lf_purchase_hist náº¿u muá»‘n tÃ­nh X1-X3 Ä‘áº§y Ä‘á»§ nháº¥t\n",
    "full_history = pl.concat([lf_purchase_val])\n",
    "\n",
    "# 2. Náº¡p Ground Truth\n",
    "gt_path = \"groundtruth_main.pkl\"\n",
    "if os.path.exists(gt_path):\n",
    "    gt_raw = joblib.load(gt_path)\n",
    "    target_users_set = set(str(u) for u in gt_raw.keys())\n",
    "    print(f\" âœ… ÄÃ£ náº¡p Ground Truth: {len(target_users_set):,} users.\")\n",
    "else:\n",
    "    print(\" âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y file groundtruth_main.pkl!\")\n",
    "\n",
    "# 3. Táº¡o báº£ng lá»‹ch sá»­ mua hÃ ng rÃºt gá»n\n",
    "# Sá»¬A Lá»–I: item_id lÃ  String, customer_id lÃ  Int32 theo Schema cá»§a báº¡n\n",
    "user_items = (\n",
    "    full_history\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").cast(pl.Int32), # Khá»›p Schema Int32\n",
    "        pl.col(\"item_id\").cast(pl.String)     # Khá»›p Schema String\n",
    "    ])\n",
    "    .drop_nulls()\n",
    "    .unique() \n",
    "    .group_by(\"customer_id\")\n",
    "    .agg(pl.col(\"item_id\").alias(\"item_id_list\"))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# 4. TÃ­nh toÃ¡n Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n (top_item_links)\n",
    "print(\" ğŸ› ï¸ Äang tÃ­nh toÃ¡n Ma tráº­n Co-occurrence...\")\n",
    "user_items_clean = user_items.explode(\"item_id_list\").rename({\"item_id_list\": \"item_id\"})\n",
    "\n",
    "# Self-join Ä‘á»ƒ tÃ¬m cÃ¡c cáº·p sáº£n pháº©m mua cÃ¹ng nhau\n",
    "item_pairs = (\n",
    "    user_items_clean.join(user_items_clean, on=\"customer_id\", suffix=\"_right\")\n",
    "    .filter(pl.col(\"item_id\") < pl.col(\"item_id_right\")) # TrÃ¡nh trÃ¹ng láº·p cáº·p (A,B) vÃ  (B,A)\n",
    "    .group_by([\"item_id\", \"item_id_right\"])\n",
    "    .len()\n",
    "    .filter(pl.col(\"len\") > 2) \n",
    ")\n",
    "\n",
    "top_item_links = (\n",
    "    pl.concat([\n",
    "        item_pairs.select([pl.col(\"item_id\"), pl.col(\"item_id_right\"), \"len\"]),\n",
    "        item_pairs.select([pl.col(\"item_id_right\").alias(\"item_id\"), pl.col(\"item_id\").alias(\"item_id_right\"), \"len\"])\n",
    "    ])\n",
    "    .sort([\"item_id\", \"len\"], descending=[False, True])\n",
    "    .group_by(\"item_id\")\n",
    "    .head(12) \n",
    "    .select([\n",
    "        pl.col(\"item_id\").alias(\"item_id_source\"), \n",
    "        pl.col(\"item_id_right\").alias(\"item_id_rec\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 5. Táº¡o Ä‘áº·c trÆ°ng phá»¥c vá»¥ Inference (infer_features_df)\n",
    "print(\" ğŸ› ï¸ Äang xÃ¢y dá»±ng infer_features_df tá»« full_history...\")\n",
    "# HÃ m nÃ y sáº½ tá»± Ä‘á»™ng nháº­n diá»‡n item_id lÃ  String vÃ  customer_id lÃ  Int32\n",
    "infer_features_df = build_features_from_purchases(\n",
    "    lf_purchases=full_history, \n",
    "    lf_items=lf_items, \n",
    "    lf_users=lf_users\n",
    ")\n",
    "\n",
    "print(\"\\nğŸš€ KHá»I Táº O XONG! CÃ¡c biáº¿n Ä‘Ã£ khá»›p Schema String/Int32.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e98904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›  Building features from Hist...\n",
      "ğŸ¯ Building labels from Recent...\n",
      "ğŸš€ Building Labels with Vectorized Hard Negative Strategy...\n",
      "ğŸ”— Merging via Entity Profile...\n",
      "âœ… Train dataset created: 18,766,660 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>X_-1</th><th>X_0</th><th>X_1</th><th>X_2</th><th>X_3</th><th>Y</th></tr><tr><td>i32</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td></tr></thead><tbody><tr><td>2057488</td><td>&quot;5140000000003&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1</td></tr><tr><td>6572298</td><td>&quot;4399000000001&quot;</td><td>1.0</td><td>69.0</td><td>1.0</td><td>1</td></tr><tr><td>5554407</td><td>&quot;2389000000003&quot;</td><td>26.0</td><td>4.0</td><td>4.0</td><td>1</td></tr><tr><td>6305870</td><td>&quot;2242000910001&quot;</td><td>2.0</td><td>189.0</td><td>2.0</td><td>1</td></tr><tr><td>7428298</td><td>&quot;0020120000014&quot;</td><td>0.0</td><td>48.0</td><td>0.0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
       "â”‚ X_-1    â”† X_0           â”† X_1  â”† X_2   â”† X_3 â”† Y   â”‚\n",
       "â”‚ ---     â”† ---           â”† ---  â”† ---   â”† --- â”† --- â”‚\n",
       "â”‚ i32     â”† str           â”† f64  â”† f64   â”† f64 â”† i8  â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
       "â”‚ 2057488 â”† 5140000000003 â”† 0.0  â”† 0.0   â”† 0.0 â”† 1   â”‚\n",
       "â”‚ 6572298 â”† 4399000000001 â”† 1.0  â”† 69.0  â”† 1.0 â”† 1   â”‚\n",
       "â”‚ 5554407 â”† 2389000000003 â”† 26.0 â”† 4.0   â”† 4.0 â”† 1   â”‚\n",
       "â”‚ 6305870 â”† 2242000910001 â”† 2.0  â”† 189.0 â”† 2.0 â”† 1   â”‚\n",
       "â”‚ 7428298 â”† 0020120000014 â”† 0.0  â”† 48.0  â”† 0.0 â”† 1   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 4. BUILDING TRAIN DATASET (CHUáº¨N HÃ“A THEO SCHEMA)\n",
    "# ======================================================================\n",
    "\n",
    "def build_features_from_purchases_fixed(lf_purchases, lf_items, lf_users=None):\n",
    "    # 1. JOIN metadata - ChÃº Ã½: item_id Ä‘á»u lÃ  String nÃªn Join ráº¥t an toÃ n\n",
    "    data_lf = lf_purchases.join(\n",
    "        lf_items.select(['item_id', 'brand', 'age_group', 'category']),\n",
    "        on='item_id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # 2. TÃNH TOÃN VÃ€ GIá»® Láº I Cá»˜T KEY\n",
    "    feature_df = (\n",
    "        data_lf\n",
    "        .select([\n",
    "            pl.col(\"customer_id\").alias(\"X_-1\"), # Giá»¯ Int32 theo Schema\n",
    "            pl.col(\"item_id\").alias(\"X_0\"),      # Giá»¯ String theo Schema\n",
    "            pl.col(\"brand\"),                     # String\n",
    "            pl.col(\"category\"),                  # String\n",
    "            pl.col(\"age_group\"),                 # String\n",
    "\n",
    "            # TÃ­nh toÃ¡n táº§n suáº¥t\n",
    "            pl.len().over([\"customer_id\", \"brand\"]).alias(\"X_1\"),\n",
    "            pl.len().over([\"customer_id\", \"age_group\"]).alias(\"X_2\"),\n",
    "            pl.len().over([\"customer_id\", \"category\"]).alias(\"X_3\"),\n",
    "        ])\n",
    "        .unique(subset=[\"X_-1\", \"X_0\"])\n",
    "        .with_columns([\n",
    "            pl.col(\"X_1\").cast(pl.Float64),\n",
    "            pl.col(\"X_2\").cast(pl.Float64),\n",
    "            pl.col(\"X_3\").cast(pl.Float64),\n",
    "        ])\n",
    "    )\n",
    "    return feature_df\n",
    "\n",
    "print(\"ğŸ›  Building features from Hist...\")\n",
    "train_features_df = build_features_from_purchases_fixed(lf_purchase_hist, lf_items)\n",
    "\n",
    "print(\"ğŸ¯ Building labels from Recent...\")\n",
    "train_label_df = build_labels(lf_purchase_hist, lf_purchase_recent, lf_items, negative_ratio=1.0)\n",
    "\n",
    "print(\"ğŸ”— Merging via Entity Profile...\")\n",
    "# ChÃº Ã½: Láº¥y Metadata gá»‘c (String) Ä‘á»ƒ Join\n",
    "train_label_with_meta = train_label_df.join(\n",
    "    lf_items.select([\"item_id\", \"brand\", \"category\", \"age_group\"]),\n",
    "    on=\"item_id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# BÆ°á»›c 4: TÃ¡ch profile (KhÃ´ng cast Int64 cho brand/category ná»¯a)\n",
    "user_brand_feat = train_features_df.select([\"X_-1\", \"brand\", \"X_1\"]).unique(subset=[\"X_-1\", \"brand\"])\n",
    "user_age_feat   = train_features_df.select([\"X_-1\", \"age_group\", \"X_2\"]).unique(subset=[\"X_-1\", \"age_group\"])\n",
    "user_cat_feat   = train_features_df.select([\"X_-1\", \"category\", \"X_3\"]).unique(subset=[\"X_-1\", \"category\"])\n",
    "\n",
    "# BÆ°á»›c 5: Join tá»•ng há»£p\n",
    "train_df = (\n",
    "    train_label_with_meta\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").alias(\"X_-1\"),\n",
    "        pl.col(\"item_id\").alias(\"X_0\"),\n",
    "        pl.col(\"label\").alias(\"Y\"),\n",
    "        \"brand\", \"category\", \"age_group\"\n",
    "    ])\n",
    "    .join(user_brand_feat, on=[\"X_-1\", \"brand\"], how=\"left\")\n",
    "    .join(user_age_feat, on=[\"X_-1\", \"age_group\"], how=\"left\")\n",
    "    .join(user_cat_feat, on=[\"X_-1\", \"category\"], how=\"left\")\n",
    "    .with_columns(\n",
    "        pl.col([\"X_1\", \"X_2\", \"X_3\"]).fill_null(0)\n",
    "    )\n",
    "    .select([\"X_-1\", \"X_0\", \"X_1\", \"X_2\", \"X_3\", \"Y\"])\n",
    ")\n",
    "\n",
    "print(f\"âœ… Train dataset created: {train_df.select(pl.len()).collect().item():,} rows.\")\n",
    "display(train_df.limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308ceba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š THá»NG KÃŠ TRAIN_DF (Tá»•ng sá»‘ dÃ²ng: 18,767,023)\n",
      "---------------------------------------------\n",
      "ğŸš« Sá»‘ dÃ²ng cÃ³ X1, X2, X3 Ä‘á»u báº±ng 0: 695,393 (3.71%)\n",
      "ğŸ”¸ Cá»™t X_1 (Brand) báº±ng 0: 4,757,716 (25.35%)\n",
      "ğŸ”¸ Cá»™t X_2 (Age Group) báº±ng 0: 4,276,449 (22.79%)\n",
      "ğŸ”¸ Cá»™t X_3 (Category) báº±ng 0: 1,698,387 (9.05%)\n",
      "---------------------------------------------\n",
      "âœ… Tá»· lá»‡ dá»¯ liá»‡u á»•n Ä‘á»‹nh. Báº¡n cÃ³ thá»ƒ tiáº¿n hÃ nh huáº¥n luyá»‡n.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CHECK DATA DENSITY (KIá»‚M TRA Sá» DÃ’NG Báº°NG 0)\n",
    "# ======================================================================\n",
    "\n",
    "# TÃ­nh toÃ¡n thá»‘ng kÃª\n",
    "check_df = train_df.select([\n",
    "    pl.len().alias(\"total_rows\"),\n",
    "    # Äáº¿m sá»‘ dÃ²ng mÃ  cáº£ 3 Ä‘áº·c trÆ°ng Ä‘á»u báº±ng 0\n",
    "    pl.struct([\"X_1\", \"X_2\", \"X_3\"])\n",
    "      .filter((pl.col(\"X_1\") == 0) & (pl.col(\"X_2\") == 0) & (pl.col(\"X_3\") == 0))\n",
    "      .count().alias(\"all_zeros\"),\n",
    "    \n",
    "    # Äáº¿m riÃªng láº» tá»«ng cá»™t\n",
    "    (pl.col(\"X_1\") == 0).sum().alias(\"X_1_zero\"),\n",
    "    (pl.col(\"X_2\") == 0).sum().alias(\"X_2_zero\"),\n",
    "    (pl.col(\"X_3\") == 0).sum().alias(\"X_3_zero\")\n",
    "]).collect()\n",
    "\n",
    "# TrÃ­ch xuáº¥t giÃ¡ trá»‹\n",
    "total = check_df[\"total_rows\"][0]\n",
    "all_zero = check_df[\"all_zeros\"][0]\n",
    "x1_z = check_df[\"X_1_zero\"][0]\n",
    "x2_z = check_df[\"X_2_zero\"][0]\n",
    "x3_z = check_df[\"X_3_zero\"][0]\n",
    "\n",
    "print(f\"ğŸ“Š THá»NG KÃŠ TRAIN_DF (Tá»•ng sá»‘ dÃ²ng: {total:,})\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"ğŸš« Sá»‘ dÃ²ng cÃ³ X1, X2, X3 Ä‘á»u báº±ng 0: {all_zero:,} ({all_zero/total*100:.2f}%)\")\n",
    "print(f\"ğŸ”¸ Cá»™t X_1 (Brand) báº±ng 0: {x1_z:,} ({x1_z/total*100:.2f}%)\")\n",
    "print(f\"ğŸ”¸ Cá»™t X_2 (Age Group) báº±ng 0: {x2_z:,} ({x2_z/total*100:.2f}%)\")\n",
    "print(f\"ğŸ”¸ Cá»™t X_3 (Category) báº±ng 0: {x3_z:,} ({x3_z/total*100:.2f}%)\") \n",
    "print(\"-\" * 45)\n",
    "\n",
    "if all_zero / total > 0.8:\n",
    "    print(\"âš ï¸ Cáº¢NH BÃO: Tá»· lá»‡ dÃ²ng trá»‘ng quÃ¡ cao! HÃ£y kiá»ƒm tra láº¡i logic táº¡o Candidate.\")\n",
    "else:\n",
    "    print(\"âœ… Tá»· lá»‡ dá»¯ liá»‡u á»•n Ä‘á»‹nh. Báº¡n cÃ³ thá»ƒ tiáº¿n hÃ nh huáº¥n luyá»‡n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b6f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Collecting and processing training data...\n",
      "ğŸš€ Training XGBoost with 18,767,339 samples...\n",
      "âœ… Model trained and saved at: models/xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 5. TRAINING MODEL (OPTIMIZED FOR X1, X2, X3 & SCHEMA)\n",
    "# ======================================================================\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "import polars as pl\n",
    "from src import config\n",
    "\n",
    "# Danh sÃ¡ch feature báº¡n Ä‘Ã£ chá»n (X1: Brand, X2: Age, X3: Cat)\n",
    "feature_cols = [\"X_1\", \"X_2\", \"X_3\"] \n",
    "\n",
    "# Thá»±c thi huáº¥n luyá»‡n\n",
    "model_xgb = train_model(train_df, feature_cols)\n",
    "\n",
    "# 4. LÆ°u mÃ´ hÃ¬nh\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "joblib.dump(model_xgb, config.MODEL_PATH)\n",
    "print(f\"âœ… Model trained and saved at: {config.MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77bf0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Táº¦M QUAN TRá»ŒNG Cá»¦A CÃC TÃNH NÄ‚NG (FEATURE IMPORTANCE):\n",
      "  - X_3: 4404.66\n",
      "  - X_1: 873.54\n",
      "  - X_2: 678.32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Láº¥y Ä‘iá»ƒm quan trá»ng cá»§a cÃ¡c tÃ­nh nÄƒng\n",
    "importance = model_xgb.get_score(importance_type='gain') # Hoáº·c 'weight'\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Hiá»ƒn thá»‹\n",
    "print(\"ğŸ“Š Táº¦M QUAN TRá»ŒNG Cá»¦A CÃC TÃNH NÄ‚NG (FEATURE IMPORTANCE):\")\n",
    "for feat, score in sorted_importance:\n",
    "    print(f\"  - {feat}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b88e7395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating Candidate Pool (Popular & Bought-together)...\n",
      "Step 2: Processing 628,069 users in batches of 5000...\n",
      "   Processed 5,000 / 628,069 users...\n",
      "   Processed 55,000 / 628,069 users...\n",
      "   Processed 105,000 / 628,069 users...\n",
      "   Processed 155,000 / 628,069 users...\n",
      "   Processed 205,000 / 628,069 users...\n",
      "   Processed 255,000 / 628,069 users...\n",
      "   Processed 305,000 / 628,069 users...\n",
      "   Processed 355,000 / 628,069 users...\n",
      "   Processed 405,000 / 628,069 users...\n",
      "   Processed 455,000 / 628,069 users...\n",
      "   Processed 505,000 / 628,069 users...\n",
      "   Processed 555,000 / 628,069 users...\n",
      "   Processed 605,000 / 628,069 users...\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 6. RUN INFERENCE FOR WARM USERS (OPTIMIZED WITH ABSOLUTE BOOSTING)\n",
    "# ======================================================================\n",
    "import xgboost as xgb\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def run_inference_v6_final(model, infer_features_df, lf_purchases, lf_items, top_item_links, user_items, top_k=10, batch_size=5000):\n",
    "    # --- BÆ¯á»šC 1: Táº O CANDIDATE POOL PHá»” BIáº¾N ---\n",
    "    print(\"Step 1: Creating Candidate Pool (Popular & Bought-together)...\")\n",
    "    popular_items_list = (\n",
    "        lf_purchases.group_by(\"item_id\")\n",
    "        .agg(pl.len().alias(\"count\"))\n",
    "        .sort(\"count\", descending=True)\n",
    "        .head(200) \n",
    "        .select(pl.col(\"item_id\").cast(pl.String))\n",
    "        .collect()\n",
    "        .get_column(\"item_id\")\n",
    "        .to_list()\n",
    "    )\n",
    "    popular_items_df = pl.DataFrame({\"X_0\": popular_items_list}).cast({\"X_0\": pl.String})\n",
    "\n",
    "    # --- BÆ¯á»šC 2: CHUáº¨N Bá»Š USER PROFILE ---\n",
    "    user_profiles = (\n",
    "        infer_features_df\n",
    "        .select([\"X_-1\", \"X_1\", \"X_2\", \"X_3\"])\n",
    "        .unique(subset=[\"X_-1\"])\n",
    "        .cast({\"X_-1\": pl.Int32})\n",
    "        .lazy()\n",
    "    )\n",
    "\n",
    "    all_users = user_profiles.select(\"X_-1\").collect().get_column(\"X_-1\").to_list()\n",
    "    print(f\"Step 2: Processing {len(all_users):,} users in batches of {batch_size}...\")\n",
    "\n",
    "    all_recommendations = []\n",
    "    item_col = \"item_id_list\" if \"item_id_list\" in user_items.columns else \"item_id\"\n",
    "\n",
    "    # --- BÆ¯á»šC 3: Dá»° ÄOÃN VÃ€ RERANK THEO BATCH ---\n",
    "    for i in range(0, len(all_users), batch_size):\n",
    "        batch_u = all_users[i : i + batch_size]\n",
    "        batch_u_df = pl.DataFrame({\"X_-1\": batch_u}).cast({\"X_-1\": pl.Int32})\n",
    "\n",
    "        # Nguá»“n A: Phá»• biáº¿n\n",
    "        c_pop = batch_u_df.join(popular_items_df, how=\"cross\")\n",
    "\n",
    "        # Nguá»“n B: Bought-together (Sá»©c máº¡nh 3.2% Precision)\n",
    "        c_bought = (\n",
    "            batch_u_df.join(user_items.rename({\"customer_id\": \"X_-1\"}).cast({\"X_-1\": pl.Int32}), on=\"X_-1\")\n",
    "            .explode(item_col)\n",
    "            .cast({item_col: pl.String})\n",
    "            .join(top_item_links.cast({\"item_id_source\": pl.String, \"item_id_rec\": pl.String}), \n",
    "                  left_on=item_col, right_on=\"item_id_source\")\n",
    "            .select([pl.col(\"X_-1\"), pl.col(\"item_id_rec\").alias(\"X_0\")])\n",
    "            .unique()\n",
    "        )\n",
    "\n",
    "        candidates_batch = pl.concat([c_pop, c_bought]).unique()\n",
    "\n",
    "        batch_data = (\n",
    "            candidates_batch.lazy()\n",
    "            .join(user_profiles, on=\"X_-1\", how=\"left\")\n",
    "            .with_columns(pl.col([\"X_1\", \"X_2\", \"X_3\"]).fill_null(0))\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "        if batch_data.height > 0:\n",
    "            X_infer = batch_data.select([\"X_1\", \"X_2\", \"X_3\"]).to_pandas()\n",
    "            probs = model.predict(xgb.DMatrix(X_infer))\n",
    "            batch_result = batch_data.with_columns(pl.Series(\"prob\", probs))\n",
    "            \n",
    "            # --- CHIáº¾N THUáº¬T RERANKING Äá»˜T PHÃ ---\n",
    "            # DÃ¹ng Bonus +100 thay vÃ¬ nhÃ¢n Multiplier Ä‘á»ƒ Ã©p Æ°u tiÃªn tuyá»‡t Ä‘á»‘i Item-Links\n",
    "            batch_result = (\n",
    "                batch_result\n",
    "                .join(c_bought.with_columns(pl.lit(100.0).alias(\"bonus\")), \n",
    "                      on=[\"X_-1\", \"X_0\"], how=\"left\")\n",
    "                .with_columns(\n",
    "                    (pl.col(\"prob\") + pl.col(\"bonus\").fill_null(0.0)).alias(\"final_score\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Lá»c Ä‘á»“ cÅ© (Anti-join) - Báº¯t buá»™c Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm sáº£n pháº©m má»›i\n",
    "            purchased = (\n",
    "                lf_purchases.filter(pl.col(\"customer_id\").cast(pl.Int32).is_in(batch_u))\n",
    "                .select([pl.col(\"customer_id\").alias(\"X_-1\").cast(pl.Int32), \n",
    "                         pl.col(\"item_id\").alias(\"X_0\").cast(pl.String)])\n",
    "                .collect()\n",
    "            )\n",
    "            \n",
    "            top_recs = (\n",
    "                batch_result.join(purchased, on=[\"X_-1\", \"X_0\"], how=\"anti\")\n",
    "                .sort([\"X_-1\", \"final_score\"], descending=[False, True])\n",
    "                .group_by(\"X_-1\")\n",
    "                .head(top_k)\n",
    "                .select([\n",
    "                    pl.col(\"X_-1\").cast(pl.String),\n",
    "                    pl.col(\"X_0\").cast(pl.String)\n",
    "                ])\n",
    "            )\n",
    "            all_recommendations.append(top_recs)\n",
    "        \n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"   Processed {i + len(batch_u):,} / {len(all_users):,} users...\")\n",
    "\n",
    "    return pl.concat(all_recommendations)\n",
    "\n",
    "# Thá»±c thi dá»± Ä‘oÃ¡n\n",
    "recommendations_warm = run_inference_v6_final(\n",
    "    model=model_xgb,\n",
    "    infer_features_df=infer_features_df,\n",
    "    lf_purchases=full_history,\n",
    "    lf_items=lf_items,\n",
    "    top_item_links=top_item_links, \n",
    "    user_items=user_items,\n",
    "    top_k=10, \n",
    "    batch_size=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7032002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Äang chuáº©n bá»‹ danh sÃ¡ch Popular Items tá»« dá»¯ liá»‡u gáº§n nháº¥t...\n",
      "âœ… ÄÃ£ táº¡o xong danh sÃ¡ch 100 mÃ³n phá»• biáº¿n nháº¥t.\n",
      "ğŸ“ VÃ­ dá»¥ 5 mÃ³n Ä‘áº§u báº£ng: ['4690000000001', '1512000000004', '2803000000013', '6768000000005', '0020020000185']\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 5. CREATE POPULAR ITEMS LIST (FOR COLD START STRATEGY)\n",
    "# ======================================================================\n",
    "\n",
    "print(\"ğŸ“¦ Äang chuáº©n bá»‹ danh sÃ¡ch Popular Items tá»« dá»¯ liá»‡u gáº§n nháº¥t...\")\n",
    "\n",
    "# NÃªn sá»­ dá»¥ng dá»¯ liá»‡u cá»§a thÃ¡ng gáº§n nháº¥t (lf_purchase_val) Ä‘á»ƒ láº¥y xu hÆ°á»›ng má»›i nháº¥t\n",
    "# Äiá»u nÃ y giÃºp tÄƒng Precision cho nhÃ³m Cold lÃªn má»©c ~0.011668\n",
    "popular_items_list = (\n",
    "    lf_purchase_val.group_by(\"item_id\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    "    .head(100) # Láº¥y 100 mÃ³n Ä‘á»ƒ lÃ m kho dá»± phÃ²ng (Candidate Pool)\n",
    "    .collect()\n",
    "    .get_column(\"item_id\")\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# Ã‰p kiá»ƒu sang String Ä‘á»ƒ Ä‘á»“ng nháº¥t vá»›i schema cá»§a Submission vÃ  Item-Links\n",
    "popular_items_list_str = [str(i) for i in popular_items_list]\n",
    "\n",
    "print(f\"âœ… ÄÃ£ táº¡o xong danh sÃ¡ch {len(popular_items_list_str)} mÃ³n phá»• biáº¿n nháº¥t.\")\n",
    "print(f\"ğŸ“ VÃ­ dá»¥ 5 mÃ³n Ä‘áº§u báº£ng: {popular_items_list_str[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df597fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] PhÃ¢n tÃ¡ch nhÃ³m Warm vÃ  Cold Users...\n",
      " âœ… Thá»‘ng kÃª: 244,990 Warm users vÃ  146,910 Cold users.\n",
      "[2/3] Äang táº¡o gá»£i Ã½ bá»• trá»£ (Repurchase & Item-Links)...\n",
      "[3/3] Äang gá»™p káº¿t quáº£ vÃ  lá»c theo Ä‘Ãºng danh sÃ¡ch má»¥c tiÃªu...\n",
      " âœ… Táº¤T Cáº¢ HOÃ€N Táº¤T!\n",
      " ğŸ“Š Tá»•ng sá»‘ User thá»±c táº¿ trong file: 391,900\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 7. HYBRID HANDLING (REPURCHASE + ITEM-LINKS + POPULAR) - FIXED VERSION\n",
    "# ======================================================================\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "print(\"[1/3] PhÃ¢n tÃ¡ch nhÃ³m Warm vÃ  Cold Users...\")\n",
    "\n",
    "# 1. Thu tháº­p káº¿t quáº£ tá»« Cell 6\n",
    "if isinstance(recommendations_warm, pl.LazyFrame):\n",
    "    warm_results = recommendations_warm.select([\n",
    "        pl.col(\"X_-1\").cast(pl.String),\n",
    "        pl.col(\"X_0\").cast(pl.String)\n",
    "    ]).collect()\n",
    "else:\n",
    "    warm_results = recommendations_warm.select([\n",
    "        pl.col(\"X_-1\").cast(pl.String),\n",
    "        pl.col(\"X_0\").cast(pl.String)\n",
    "    ])\n",
    "\n",
    "warm_users_set = set(warm_results.get_column(\"X_-1\").unique().to_list())\n",
    "cold_users_ids = list(target_users_set - warm_users_set)\n",
    "print(f\" âœ… Thá»‘ng kÃª: {len(warm_users_set & target_users_set):,} Warm users vÃ  {len(cold_users_ids):,} Cold users.\")\n",
    "\n",
    "# --- 2. Sinh gá»£i Ã½ bá»• trá»£ Ä‘a táº§ng ---\n",
    "print(\"[2/3] Äang táº¡o gá»£i Ã½ bá»• trá»£ (Repurchase & Item-Links)...\")\n",
    "df_cold_base = pl.DataFrame({\"X_-1\": [str(u) for u in cold_users_ids]}).lazy()\n",
    "\n",
    "# NhÃ¡nh A: Repurchase (Äá»“ cÅ© cho 47k users)\n",
    "batch_repur = (\n",
    "    df_cold_base.join(\n",
    "        user_items.lazy().with_columns(pl.col(\"customer_id\").cast(pl.String)).rename({\"customer_id\": \"X_-1\"}),\n",
    "        on=\"X_-1\", how=\"inner\"\n",
    "    )\n",
    "    .explode(\"item_id_list\")\n",
    "    .select([pl.col(\"X_-1\"), pl.col(\"item_id_list\").alias(\"X_0\")])\n",
    ")\n",
    "\n",
    "# NhÃ¡nh B: Semi-cold (Äá»“ mua kÃ¨m)\n",
    "batch_semi = (\n",
    "    df_cold_base.join(\n",
    "        user_items.lazy().with_columns(pl.col(\"customer_id\").cast(pl.String)).rename({\"customer_id\": \"X_-1\"}), \n",
    "        on=\"X_-1\", how=\"inner\"\n",
    "    )\n",
    "    .explode(\"item_id_list\")\n",
    "    .rename({\"item_id_list\": \"item_id\"})\n",
    "    .join(\n",
    "        top_item_links.lazy().with_columns([\n",
    "            pl.col(\"item_id_source\").cast(pl.String),\n",
    "            pl.col(\"item_id_rec\").cast(pl.String)\n",
    "        ]), \n",
    "        left_on=\"item_id\", right_on=\"item_id_source\", how=\"inner\"\n",
    "    )\n",
    "    .select([pl.col(\"X_-1\"), pl.col(\"item_id_rec\").alias(\"X_0\")])\n",
    ")\n",
    "\n",
    "# NhÃ¡nh C: Pure-cold (Popular)\n",
    "users_with_any_rec = pl.concat([\n",
    "    batch_semi.select(\"X_-1\").unique(),\n",
    "    batch_repur.select(\"X_-1\").unique()\n",
    "]).unique()\n",
    "\n",
    "batch_pure = (\n",
    "    df_cold_base.join(users_with_any_rec, on=\"X_-1\", how=\"anti\")\n",
    "    .with_columns(pl.lit(popular_items_list_str).alias(\"X_0\"))\n",
    "    .explode(\"X_0\")\n",
    "    .select([\"X_-1\", \"X_0\"])\n",
    ")\n",
    "\n",
    "# Gá»™p káº¿t quáº£ Cold Start vá»›i thá»© tá»± Æ°u tiÃªn\n",
    "recommendations_cold = pl.concat([\n",
    "    batch_repur.with_columns(pl.lit(1).alias(\"sub_prio\")),\n",
    "    batch_semi.with_columns(pl.lit(2).alias(\"sub_prio\")),\n",
    "    batch_pure.with_columns(pl.lit(3).alias(\"sub_prio\"))\n",
    "]).unique(subset=[\"X_-1\", \"X_0\"]).sort([\"X_-1\", \"sub_prio\"]).collect()\n",
    "\n",
    "# --- 3. Káº¾T Há»¢P VÃ€ XUáº¤T SUBMISSION (Ã‰P THEO GT_DICT) ---\n",
    "print(\"[3/3] Äang gá»™p káº¿t quáº£ vÃ  lá»c theo Ä‘Ãºng danh sÃ¡ch má»¥c tiÃªu...\")\n",
    "\n",
    "# Láº¥y danh sÃ¡ch ID chuáº©n tá»« Ground Truth (ÄÃ¡p Ã¡n cá»§a tháº§y)\n",
    "final_target_ids = list(gt_dict.keys())\n",
    "\n",
    "submission = (\n",
    "    pl.concat([\n",
    "        warm_results.with_columns(pl.lit(1).alias(\"prio\")),\n",
    "        recommendations_cold.select([\"X_-1\", \"X_0\"]).with_columns(pl.lit(2).alias(\"prio\"))\n",
    "    ])\n",
    "    .cast({\"X_-1\": pl.String})\n",
    "    # BÆ¯á»šC QUAN TRá»ŒNG: Chá»‰ giá»¯ láº¡i nhá»¯ng ngÆ°á»i cÃ³ trong Ä‘Ã¡p Ã¡n thá»±c táº¿\n",
    "    .filter(pl.col(\"X_-1\").is_in(final_target_ids)) \n",
    "    .unique(subset=[\"X_-1\", \"X_0\"]) \n",
    "    .sort([\"X_-1\", \"prio\"]) \n",
    "    .group_by(\"X_-1\")\n",
    "    .agg(pl.col(\"X_0\").head(12)) \n",
    "    .with_columns(\n",
    "        pl.col(\"X_0\").list.join(\" \").alias(\"prediction\")\n",
    "    )\n",
    "    .select([\n",
    "        pl.col(\"X_-1\").alias(\"customer_id\"),\n",
    "        \"prediction\"\n",
    "    ])\n",
    "    .unique(subset=[\"customer_id\"]) \n",
    ")\n",
    "\n",
    "# Ghi file\n",
    "submission.write_csv(\"final_submission.csv\")\n",
    "\n",
    "print(f\" âœ… Táº¤T Cáº¢ HOÃ€N Táº¤T!\")\n",
    "print(f\" ğŸ“Š Tá»•ng sá»‘ User thá»±c táº¿ trong file: {submission.height:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69c2fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Äang náº¡p Ground Truth tá»« file groundtruth_main.pkl...\n",
      "âœ… ÄÃ£ náº¡p xong 391,900 users trong Ground Truth.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Náº¡p file pkl chá»©a Ä‘Ã¡p Ã¡n thá»±c táº¿\n",
    "print(\"ğŸ“‚ Äang náº¡p Ground Truth tá»« file groundtruth_main.pkl...\")\n",
    "gt_dict_raw = joblib.load(\"groundtruth_main.pkl\")\n",
    "\n",
    "# Chuyá»ƒn Ä‘á»•i key sang String Ä‘á»ƒ Ä‘áº£m báº£o khá»›p vá»›i customer_id trong submission\n",
    "gt_dict = {str(k): v for k, v in gt_dict_raw.items()}\n",
    "\n",
    "print(f\"âœ… ÄÃ£ náº¡p xong {len(gt_dict):,} users trong Ground Truth.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b3946f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ chuáº©n bá»‹ pred_dict cho 391,900 users.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# 1. Äá»c láº¡i file káº¿t quáº£ vá»«a xuáº¥t ra\n",
    "df_pred = pl.read_csv(\"final_submission.csv\")\n",
    "\n",
    "# 2. Chuyá»ƒn Ä‘á»•i tá»« báº£ng sang Dictionary Ä‘á»ƒ tra cá»©u nhanh trong hÃ m Ä‘Ã¡nh giÃ¡\n",
    "# Format: { \"customer_id\": [\"item1\", \"item2\", ... , \"item12\"] }\n",
    "pred_dict = {\n",
    "    str(row[\"customer_id\"]): row[\"prediction\"].split(\" \") \n",
    "    for row in df_pred.iter_rows(named=True)\n",
    "}\n",
    "\n",
    "print(f\"âœ… ÄÃ£ chuáº©n bá»‹ pred_dict cho {len(pred_dict):,} users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8495426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang táº¡o hist_lookup tá»« dá»¯ liá»‡u lá»‹ch sá»­...\n",
      "âœ… ÄÃ£ chuáº©n bá»‹ xong lá»‹ch sá»­ cho 628,069 khÃ¡ch hÃ ng.\n"
     ]
    }
   ],
   "source": [
    "# --- Táº O HIST_LOOKUP (Sá»¬A Lá»–I .COLLECT) ---\n",
    "print(\"ğŸ” Äang táº¡o hist_lookup tá»« dá»¯ liá»‡u lá»‹ch sá»­...\")\n",
    "\n",
    "# Kiá»ƒm tra náº¿u user_items lÃ  LazyFrame thÃ¬ má»›i dÃ¹ng .collect()\n",
    "if isinstance(user_items, pl.LazyFrame):\n",
    "    user_items_df = user_items.select([\"customer_id\", \"item_id_list\"]).collect()\n",
    "else:\n",
    "    user_items_df = user_items.select([\"customer_id\", \"item_id_list\"])\n",
    "\n",
    "# Táº¡o Dictionary Ä‘á»ƒ tra cá»©u\n",
    "hist_lookup = {\n",
    "    str(row[\"customer_id\"]): set(map(str, row[\"item_id_list\"]))\n",
    "    for row in user_items_df.iter_rows(named=True)\n",
    "}\n",
    "\n",
    "print(f\"âœ… ÄÃ£ chuáº©n bá»‹ xong lá»‹ch sá»­ cho {len(hist_lookup):,} khÃ¡ch hÃ ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "203840be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ CHáº¾ Äá»˜ ÄÃNH GIÃ: Báº¬T (TÃ­nh cáº£ 47k user mua Ä‘á»“ cÅ©)\n",
      "============================================================\n",
      "ğŸ”¥ NHÃ“M WARM      : 244,990 users - P@10: 0.019017\n",
      "âš¡ NHÃ“M SEMI-COLD : 0 users - P@10: nan\n",
      "â„ï¸ NHÃ“M PURE-COLD : 146,910 users - P@10: 0.005970\n",
      "------------------------------------------------------------\n",
      "ğŸ† GLOBAL MEAN P@10 : 0.014126\n",
      "ğŸ“¦ Tá»•ng User Ä‘Æ°á»£c tÃ­nh: 391,900 / 391,900\n",
      "â„¹ï¸ Trong Ä‘Ã³ cÃ³ 47,470 users lÃ  nhÃ³m 'Chá»‰ mua láº¡i Ä‘á»“ cÅ©'.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\my_rec_system\\venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\HP\\Desktop\\my_rec_system\\venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_flexible(pred_dict, gt_dict, hist_lookup, warm_users_trained, K=10, include_repurchase=True):\n",
    "    warm_precs = []\n",
    "    semi_precs = []\n",
    "    pure_precs = []\n",
    "    \n",
    "    # 47k users nÃ y thÆ°á»ng thuá»™c nhÃ³m Warm/Semi-cold vÃ¬ há» cÃ³ lá»‹ch sá»­\n",
    "    repurchase_only_count = 0 \n",
    "\n",
    "    for user_id, gt_items in gt_dict.items():\n",
    "        u_str = str(user_id)\n",
    "        current_gt = set(map(str, gt_items))\n",
    "        past_items = hist_lookup.get(u_str, set())\n",
    "        \n",
    "        # XÃ¡c Ä‘á»‹nh táº­p Ä‘á»“ má»›i\n",
    "        relevant_new_items = current_gt - past_items\n",
    "        \n",
    "        # LOGIC SIÃŠU THAM Sá»:\n",
    "        if not relevant_new_items:\n",
    "            if not include_repurchase:\n",
    "                # Táº¯t: Bá» qua 47k users chá»‰ mua Ä‘á»“ cÅ©\n",
    "                continue \n",
    "            else:\n",
    "                # Báº­t: Coi Ä‘á»“ cÅ© khÃ¡ch mua láº¡i lÃ  má»¥c tiÃªu Ä‘Ã¡nh giÃ¡ (Ground Truth)\n",
    "                target_items = current_gt\n",
    "                repurchase_only_count += 1\n",
    "        else:\n",
    "            # LuÃ´n tÃ­nh nhá»¯ng ngÆ°á»i cÃ³ mua Ä‘á»“ má»›i\n",
    "            target_items = relevant_new_items\n",
    "\n",
    "        # TÃ­nh Precision@K\n",
    "        user_preds = pred_dict.get(u_str, [])[:K]\n",
    "        if not user_preds:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            hits = len(set(user_preds) & target_items)\n",
    "            precision = hits / K\n",
    "        \n",
    "        # PhÃ¢n luá»“ng bÃ¡o cÃ¡o dá»±a trÃªn tráº¡ng thÃ¡i User\n",
    "        if u_str in warm_users_trained:\n",
    "            warm_precs.append(precision)\n",
    "        elif u_str in hist_lookup:\n",
    "            semi_precs.append(precision)\n",
    "        else:\n",
    "            pure_precs.append(precision)\n",
    "            \n",
    "    return warm_precs, semi_precs, pure_precs, repurchase_only_count\n",
    "\n",
    "# --- CHáº Y THá»¬ NGHIá»†M ---\n",
    "# Báº¡n cÃ³ thá»ƒ thay Ä‘á»•i include_repurchase = True hoáº·c False á»Ÿ Ä‘Ã¢y\n",
    "include_47k = True \n",
    "\n",
    "warm_p, semi_p, pure_p, re_count = evaluate_flexible(\n",
    "    pred_dict=pred_dict, \n",
    "    gt_dict=gt_dict, \n",
    "    hist_lookup=hist_lookup,\n",
    "    warm_users_trained=warm_users_set,\n",
    "    K=10,\n",
    "    include_repurchase=include_47k\n",
    ")\n",
    "\n",
    "# --- IN BÃO CÃO ---\n",
    "status = \"Báº¬T (TÃ­nh cáº£ 47k user mua Ä‘á»“ cÅ©)\" if include_47k else \"Táº®T (Chá»‰ tÃ­nh Ä‘á»“ má»›i)\"\n",
    "print(f\"\\nğŸš€ CHáº¾ Äá»˜ ÄÃNH GIÃ: {status}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ”¥ NHÃ“M WARM      : {len(warm_p):,} users - P@10: {np.mean(warm_p):.6f}\")\n",
    "print(f\"âš¡ NHÃ“M SEMI-COLD : {len(semi_p):,} users - P@10: {np.mean(semi_p):.6f}\")\n",
    "print(f\"â„ï¸ NHÃ“M PURE-COLD : {len(pure_p):,} users - P@10: {np.mean(pure_p):.6f}\")\n",
    "print(\"-\" * 60)\n",
    "total_evaluated = len(warm_p) + len(semi_p) + len(pure_p)\n",
    "print(f\"ğŸ† GLOBAL MEAN P@10 : {np.mean(warm_p + semi_p + pure_p):.6f}\")\n",
    "print(f\"ğŸ“¦ Tá»•ng User Ä‘Æ°á»£c tÃ­nh: {total_evaluated:,} / {len(gt_dict):,}\")\n",
    "if include_47k:\n",
    "    print(f\"â„¹ï¸ Trong Ä‘Ã³ cÃ³ {re_count:,} users lÃ  nhÃ³m 'Chá»‰ mua láº¡i Ä‘á»“ cÅ©'.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
