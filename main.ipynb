{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a923da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import joblib\n",
    "from src.data_loader import get_all_data, split_date_lazy\n",
    "from src.feature_engineering import build_features_from_purchases, build_labels\n",
    "from src.trainer import train_model\n",
    "from src import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7ecbfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading and splitting data...\n",
      "ğŸ“‚ Äang quÃ©t dá»¯ liá»‡u tá»« thÆ° má»¥c: c:\\Users\\HP\\Desktop\\my_rec_system\\data\n",
      "âœ… ÄÃ£ load Items\n",
      "âœ… ÄÃ£ load Users\n",
      "âœ… ÄÃ£ load Purchases\n",
      "âœ… Data split completed.\n",
      "   - Hist samples: 29,580,965\n",
      "   - Recent samples: 3,099,667\n",
      "   - Val samples: 3,049,193\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 2. LOAD & SPLIT DATA USING CUSTOM UTILITY\n",
    "# ======================================================================\n",
    "print(\"ğŸ“‚ Loading and splitting data...\")\n",
    "\n",
    "# 1. Load dá»¯ liá»‡u\n",
    "lf_items, lf_users, lf_purchases = get_all_data(\"data\")\n",
    "\n",
    "# 2. Sá»­ dá»¥ng hÃ m cá»§a báº¡n Ä‘á»ƒ chia táº­p\n",
    "# HÃ m nÃ y tá»± Ä‘á»™ng xá»­ lÃ½ cast datetime náº¿u created_date Ä‘ang lÃ  String\n",
    "lf_purchase_hist, lf_purchase_recent, lf_purchase_val = split_date_lazy(\n",
    "    lf_purchases, \n",
    "    date_column_name=\"created_date\"\n",
    ")\n",
    "\n",
    "# 3. Kiá»ƒm tra nhanh káº¿t quáº£ báº±ng collect_schema (khÃ´ng tá»‘n RAM)\n",
    "print(\"âœ… Data split completed.\")\n",
    "print(f\"   - Hist samples: {lf_purchase_hist.select(pl.len()).collect().item():,}\")\n",
    "print(f\"   - Recent samples: {lf_purchase_recent.select(pl.len()).collect().item():,}\")\n",
    "print(f\"   - Val samples: {lf_purchase_val.select(pl.len()).collect().item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8961326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›  Building features from Hist...\n",
      "ğŸ¯ Building labels from Recent...\n",
      "======================================================================\n",
      "BUILD LABELS\n",
      "======================================================================\n",
      "Step 1 â€” Loading positives...\n",
      " â†’ Positives: 2,749,181\n",
      "Step 3 â€” Building exclusion map...\n",
      "Step 4 â€” Loading popular items...\n",
      "Step 5 â€” Sampling negatives...\n",
      "   Processed 100,000 users...\n",
      "   Processed 200,000 users...\n",
      "   Processed 300,000 users...\n",
      "   Processed 400,000 users...\n",
      "   Processed 500,000 users...\n",
      "   Processed 600,000 users...\n",
      "Step 6 â€” Finalizing dataset...\n",
      "Total time: 1326.14s\n",
      "======================================================================\n",
      "ğŸ”— Merging Features and Labels...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>X_-1</th><th>X_0</th><th>Y</th><th>X_1</th><th>X_2</th><th>X_3</th><th>X_4</th><th>X_5</th></tr><tr><td>i32</td><td>str</td><td>i8</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>7029481</td><td>&quot;2346000000132&quot;</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>4777120</td><td>&quot;5435000000002&quot;</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>6022658</td><td>&quot;3491000000031&quot;</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1928555</td><td>&quot;0029010010002&quot;</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>8031844</td><td>&quot;3511000000022&quot;</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
       "â”‚ X_-1    â”† X_0           â”† Y   â”† X_1 â”† X_2 â”† X_3 â”† X_4 â”† X_5 â”‚\n",
       "â”‚ ---     â”† ---           â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”‚\n",
       "â”‚ i32     â”† str           â”† i8  â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
       "â”‚ 7029481 â”† 2346000000132 â”† 1   â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”‚\n",
       "â”‚ 4777120 â”† 5435000000002 â”† 1   â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”‚\n",
       "â”‚ 6022658 â”† 3491000000031 â”† 0   â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”‚\n",
       "â”‚ 1928555 â”† 0029010010002 â”† 0   â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”‚\n",
       "â”‚ 8031844 â”† 3511000000022 â”† 0   â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 3. FEATURE ENGINEERING & LABELING\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Táº¡o Äáº·c trÆ°ng tá»« quÃ¡ khá»© (DÃ¹ng Hist Ä‘á»ƒ khÃ´ng bá»‹ Leak)\n",
    "print(\"ğŸ›  Building features from Hist...\")\n",
    "train_features_df = build_features_from_purchases(lf_purchase_hist, lf_items, lf_users)\n",
    "\n",
    "# 2. Táº¡o NhÃ£n tá»« giai Ä‘oáº¡n tiáº¿p theo (Recent lÃ m Positive)\n",
    "print(\"ğŸ¯ Building labels from Recent...\")\n",
    "train_label_df = build_labels(\n",
    "    baseHist=lf_purchase_hist, \n",
    "    labelHist=lf_purchase_recent, \n",
    "    items=lf_items,\n",
    "    negative_ratio=1.0  # Báº¡n cÃ³ thá»ƒ tÄƒng lÃªn 2.0 hoáº·c 3.0 Ä‘á»ƒ cÃ³ nhiá»u máº«u Ã¢m hÆ¡n\n",
    ")\n",
    "\n",
    "# 3. Káº¿t há»£p Features vÃ  Labels thÃ nh táº­p Train hoÃ n chá»‰nh\n",
    "print(\"ğŸ”— Merging Features and Labels...\")\n",
    "train_df = (\n",
    "    train_label_df\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").alias(\"X_-1\"),\n",
    "        pl.col(\"item_id\").alias(\"X_0\"),\n",
    "        pl.col(\"label\").alias(\"Y\")\n",
    "    ])\n",
    "    .join(train_features_df, on=[\"X_-1\", \"X_0\"], how=\"left\")\n",
    "    # Äiá»n 0 cho nhá»¯ng cáº·p (User, Item) khÃ´ng cÃ³ trong lá»‹ch sá»­ (thÆ°á»ng lÃ  máº«u Ã¢m)\n",
    "    .with_columns(\n",
    "        pl.all().exclude([\"X_-1\", \"X_0\", \"Y\"]).fill_null(0)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Soi thá»­ dá»¯ liá»‡u\n",
    "display(train_df.limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e98904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting training with features: ['X_1', 'X_2', 'X_3', 'X_4', 'X_5']\n",
      "Training XGBoost...\n",
      "âœ… Model trained and saved at: models/xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 4. EXECUTE TRAINING\n",
    "# ======================================================================\n",
    "from src.trainer import train_model\n",
    "import joblib\n",
    "from src import config\n",
    "\n",
    "# XÃ¡c Ä‘á»‹nh danh sÃ¡ch feature (khá»›p vá»›i file feature_engineering.py)\n",
    "feature_cols = [\"X_1\", \"X_2\", \"X_3\", \"X_4\", \"X_5\"]\n",
    "\n",
    "print(f\"ğŸš€ Starting training with features: {feature_cols}\")\n",
    "\n",
    "# Gá»i hÃ m tá»« trainer.py\n",
    "model_xgb = train_model(train_df, feature_cols)\n",
    "\n",
    "# LÆ°u mÃ´ hÃ¬nh vÃ o thÆ° má»¥c model/\n",
    "import os\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "joblib.dump(model_xgb, config.MODEL_PATH)\n",
    "print(f\"âœ… Model trained and saved at: {config.MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b6f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›  Building features for Jan 2025 Inference...\n",
      "âœ… Created inference features for 628,069 users.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>X_-1</th><th>X_0</th><th>X_1</th><th>X_2</th><th>X_3</th><th>X_4</th><th>X_5</th></tr><tr><td>i32</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>4895259</td><td>&quot;2808000000001&quot;</td><td>1.0</td><td>8.0</td><td>1.0</td><td>9.127946</td><td>9.124129</td></tr><tr><td>4192337</td><td>&quot;0950000000057&quot;</td><td>2.0</td><td>4.0</td><td>1.0</td><td>1.273278</td><td>2.772589</td></tr><tr><td>620605</td><td>&quot;6235000000002&quot;</td><td>1.0</td><td>1.0</td><td>1.0</td><td>2.373333</td><td>8.004366</td></tr><tr><td>3799514</td><td>&quot;2408000000010&quot;</td><td>3.0</td><td>3.0</td><td>3.0</td><td>0.585487</td><td>6.84588</td></tr><tr><td>1304875</td><td>&quot;3286000000004&quot;</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.782017</td><td>8.343316</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 7)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ X_-1    â”† X_0           â”† X_1 â”† X_2 â”† X_3 â”† X_4      â”† X_5      â”‚\n",
       "â”‚ ---     â”† ---           â”† --- â”† --- â”† --- â”† ---      â”† ---      â”‚\n",
       "â”‚ i32     â”† str           â”† f64 â”† f64 â”† f64 â”† f64      â”† f64      â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 4895259 â”† 2808000000001 â”† 1.0 â”† 8.0 â”† 1.0 â”† 9.127946 â”† 9.124129 â”‚\n",
       "â”‚ 4192337 â”† 0950000000057 â”† 2.0 â”† 4.0 â”† 1.0 â”† 1.273278 â”† 2.772589 â”‚\n",
       "â”‚ 620605  â”† 6235000000002 â”† 1.0 â”† 1.0 â”† 1.0 â”† 2.373333 â”† 8.004366 â”‚\n",
       "â”‚ 3799514 â”† 2408000000010 â”† 3.0 â”† 3.0 â”† 3.0 â”† 0.585487 â”† 6.84588  â”‚\n",
       "â”‚ 1304875 â”† 3286000000004 â”† 1.0 â”† 1.0 â”† 1.0 â”† 1.782017 â”† 8.343316 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 4.5 PREPARE INFERENCE FEATURES (TEST POOL) - FIXED\n",
    "# ======================================================================\n",
    "print(\"ğŸ›  Building features for Jan 2025 Inference...\")\n",
    "\n",
    "# 1. Gá»™p toÃ n bá»™ dá»¯ liá»‡u lá»‹ch sá»­\n",
    "full_history = pl.concat([lf_purchase_val])\n",
    "\n",
    "# 2. Táº¡o Ä‘áº·c trÆ°ng (X_1, X_2, X_3)\n",
    "from src.feature_engineering import build_features_from_purchases\n",
    "\n",
    "infer_features_df = build_features_from_purchases(\n",
    "    lf_purchases=full_history, \n",
    "    lf_items=lf_items, \n",
    "    lf_users=lf_users\n",
    ")\n",
    "\n",
    "# Sá»¬A Lá»–I Táº I ÄÃ‚Y: DÃ¹ng select vÃ  n_unique trong Lazy mode\n",
    "count_user = (\n",
    "    infer_features_df\n",
    "    .select(pl.col(\"X_-1\").n_unique())\n",
    "    .collect()\n",
    "    .item()\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created inference features for {count_user:,} users.\")\n",
    "\n",
    "# Soi thá»­ báº£ng Ä‘áº·c trÆ°ng\n",
    "display(infer_features_df.limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef43f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items = (\n",
    "    full_history\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").cast(pl.Int64), \n",
    "        pl.col(\"item_id\").cast(pl.Int64)\n",
    "    ])\n",
    "    .drop_nulls()\n",
    "    .unique() \n",
    "    .group_by(\"customer_id\")\n",
    "    .agg(pl.col(\"item_id\").alias(\"item_id_list\"))\n",
    "    .collect()\n",
    ")\n",
    "user_items_clean = (\n",
    "    user_items\n",
    "    .explode(\"item_id_list\")\n",
    "    .rename({\"item_id_list\": \"item_id\"})\n",
    "    .cast({\"item_id\": pl.Int64})\n",
    ")\n",
    "\n",
    "item_pairs = (\n",
    "    user_items_clean.join(user_items_clean, on=\"customer_id\", suffix=\"_right\")\n",
    "    .filter(pl.col(\"item_id\") < pl.col(\"item_id_right\"))\n",
    "    .group_by([\"item_id\", \"item_id_right\"])\n",
    "    .len()\n",
    "    .filter(pl.col(\"len\") > 2)\n",
    ")\n",
    "\n",
    "top_item_links = (\n",
    "    pl.concat([\n",
    "        item_pairs.select([pl.col(\"item_id\"), pl.col(\"item_id_right\"), \"len\"]),\n",
    "        item_pairs.select([pl.col(\"item_id_right\").alias(\"item_id\"), pl.col(\"item_id\").alias(\"item_id_right\"), \"len\"])\n",
    "    ])\n",
    "    .sort([\"item_id\", \"len\"], descending=[False, True])\n",
    "    .group_by(\"item_id\")\n",
    "    .head(10)\n",
    "    .select([\n",
    "        pl.col(\"item_id\").alias(\"item_id_source\"), \n",
    "        pl.col(\"item_id_right\").alias(\"item_id_rec\")\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c313ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Creating Hybrid Candidate Pool...\n",
      "Step 2: Processing 628,069 users in batches of 2000...\n",
      "   Processed 2,000 / 628,069 users...\n",
      "   Processed 22,000 / 628,069 users...\n",
      "   Processed 42,000 / 628,069 users...\n",
      "   Processed 62,000 / 628,069 users...\n",
      "   Processed 82,000 / 628,069 users...\n",
      "   Processed 102,000 / 628,069 users...\n",
      "   Processed 122,000 / 628,069 users...\n",
      "   Processed 142,000 / 628,069 users...\n",
      "   Processed 162,000 / 628,069 users...\n",
      "   Processed 182,000 / 628,069 users...\n",
      "   Processed 202,000 / 628,069 users...\n",
      "   Processed 222,000 / 628,069 users...\n",
      "   Processed 242,000 / 628,069 users...\n",
      "   Processed 262,000 / 628,069 users...\n",
      "   Processed 282,000 / 628,069 users...\n",
      "   Processed 302,000 / 628,069 users...\n",
      "   Processed 322,000 / 628,069 users...\n",
      "   Processed 342,000 / 628,069 users...\n",
      "   Processed 362,000 / 628,069 users...\n",
      "   Processed 382,000 / 628,069 users...\n",
      "   Processed 402,000 / 628,069 users...\n",
      "   Processed 422,000 / 628,069 users...\n",
      "   Processed 442,000 / 628,069 users...\n",
      "   Processed 462,000 / 628,069 users...\n",
      "   Processed 482,000 / 628,069 users...\n",
      "   Processed 502,000 / 628,069 users...\n",
      "   Processed 522,000 / 628,069 users...\n",
      "   Processed 542,000 / 628,069 users...\n",
      "   Processed 562,000 / 628,069 users...\n",
      "   Processed 582,000 / 628,069 users...\n",
      "   Processed 602,000 / 628,069 users...\n",
      "   Processed 622,000 / 628,069 users...\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def run_inference_local_v4(model, infer_features_df, lf_purchases, top_item_links, user_items, top_k=10, batch_size=2000):\n",
    "    # 1. Candidate Pool: Top 400 mÃ³n phá»• biáº¿n (Eager)\n",
    "    print(\"Step 1: Creating Hybrid Candidate Pool...\")\n",
    "    popular_items_list = (\n",
    "        lf_purchases.group_by(\"item_id\")\n",
    "        .agg(pl.len().alias(\"count\"))\n",
    "        .sort(\"count\", descending=True)\n",
    "        .head(400) \n",
    "        .select(pl.col(\"item_id\").cast(pl.Int64))\n",
    "        .collect()\n",
    "        .get_column(\"item_id\")\n",
    "        .to_list()\n",
    "    )\n",
    "    popular_items_df = pl.DataFrame({\"X_0\": popular_items_list})\n",
    "\n",
    "    # 2. Chuáº©n bá»‹ danh sÃ¡ch User\n",
    "    all_users = (\n",
    "        infer_features_df\n",
    "        .select(pl.col(\"X_-1\").cast(pl.Int64))\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .get_column(\"X_-1\")\n",
    "        .to_list()\n",
    "    )\n",
    "    print(f\"Step 2: Processing {len(all_users):,} users in batches of {batch_size}...\")\n",
    "\n",
    "    all_recommendations = []\n",
    "    \n",
    "    # Ã‰p kiá»ƒu features sang Int64 vÃ  Ä‘á»ƒ á»Ÿ dáº¡ng Lazy Ä‘á»ƒ join tá»‘i Æ°u\n",
    "    lazy_features = infer_features_df.with_columns([\n",
    "        pl.col(\"X_-1\").cast(pl.Int64),\n",
    "        pl.col(\"X_0\").cast(pl.Int64)\n",
    "    ]).lazy()\n",
    "\n",
    "    # XÃ¡c Ä‘á»‹nh tÃªn cá»™t item chÃ­nh xÃ¡c trong user_items\n",
    "    item_col = \"item_id_list\" if \"item_id_list\" in user_items.columns else \"item_id\"\n",
    "\n",
    "    for i in range(0, len(all_users), batch_size):\n",
    "        batch_u = all_users[i : i + batch_size]\n",
    "        batch_u_df = pl.DataFrame({\"X_-1\": batch_u})\n",
    "\n",
    "        # --- NGUá»’N A: Popular (400 mÃ³n) ---\n",
    "        c_pop = batch_u_df.join(popular_items_df, how=\"cross\")\n",
    "\n",
    "        # --- NGUá»’N B: Bought-together (CÃ¡ nhÃ¢n hÃ³a - Eager) ---\n",
    "        c_bought = (\n",
    "            batch_u_df.join(user_items.rename({\"customer_id\": \"X_-1\"}), on=\"X_-1\")\n",
    "            .explode(item_col)\n",
    "            .cast({item_col: pl.Int64})\n",
    "            .join(top_item_links, left_on=item_col, right_on=\"item_id_source\")\n",
    "            .select([pl.col(\"X_-1\"), pl.col(\"item_id_rec\").alias(\"X_0\")])\n",
    "            .unique()\n",
    "        )\n",
    "\n",
    "        # Gá»™p á»©ng viÃªn vÃ  chuyá»ƒn sang Lazy Ä‘á»ƒ Join Feature\n",
    "        # c_pop vÃ  c_bought Ä‘á»u lÃ  DataFrame, concat xong chuyá»ƒn sang lazy\n",
    "        candidates_lazy = pl.concat([c_pop, c_bought]).unique().lazy()\n",
    "\n",
    "        # Join Features & Predict\n",
    "        batch_data = (\n",
    "            candidates_lazy\n",
    "            .join(lazy_features.filter(pl.col(\"X_-1\").is_in(batch_u)), on=[\"X_-1\", \"X_0\"], how=\"left\")\n",
    "            .with_columns(pl.all().exclude([\"X_-1\", \"X_0\"]).fill_null(0))\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "        # XGBoost Predict\n",
    "        X_infer = batch_data.select([\"X_1\", \"X_2\", \"X_3\", \"X_4\", \"X_5\"]).to_pandas()\n",
    "        probs = model.predict(xgb.DMatrix(X_infer))\n",
    "        batch_result = batch_data.with_columns(pl.Series(\"prob\", probs))\n",
    "        \n",
    "        # --- CHIáº¾N THUáº¬T RERANKING: Æ¯u tiÃªn nháº¹ cÃ¡c mÃ³n Bought-together ---\n",
    "        # Sá»¬A Lá»–I: c_bought Ä‘Ã£ lÃ  DataFrame nÃªn KHÃ”NG gá»i .collect() ná»¯a\n",
    "        batch_result = batch_result.join(\n",
    "            c_bought.with_columns(pl.lit(1.15).alias(\"multiplier\")), \n",
    "            on=[\"X_-1\", \"X_0\"], \n",
    "            how=\"left\"\n",
    "        ).with_columns(\n",
    "            (pl.col(\"prob\") * pl.col(\"multiplier\").fill_null(1.0)).alias(\"final_score\")\n",
    "        )\n",
    "\n",
    "        # Lá»c mÃ³n Ä‘Ã£ mua\n",
    "        purchased = (\n",
    "            lf_purchases.filter(pl.col(\"customer_id\").cast(pl.Int64).is_in(batch_u))\n",
    "            .select([pl.col(\"customer_id\").alias(\"X_-1\").cast(pl.Int64), pl.col(\"item_id\").alias(\"X_0\").cast(pl.Int64)])\n",
    "            .collect()\n",
    "        )\n",
    "        \n",
    "        top_recs = (\n",
    "            batch_result.join(purchased, on=[\"X_-1\", \"X_0\"], how=\"anti\")\n",
    "            .sort([\"X_-1\", \"final_score\"], descending=[False, True])\n",
    "            .group_by(\"X_-1\")\n",
    "            .head(top_k)\n",
    "            .select([\"X_-1\", \"X_0\"])\n",
    "        )\n",
    "        \n",
    "        all_recommendations.append(top_recs)\n",
    "        \n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"   Processed {i + len(batch_u):,} / {len(all_users):,} users...\")\n",
    "\n",
    "    return pl.concat(all_recommendations)\n",
    "\n",
    "# --- THá»°C THI ---\n",
    "recommendations_warm = run_inference_local_v4(\n",
    "    model=model_xgb,\n",
    "    infer_features_df=infer_features_df,\n",
    "    lf_purchases=full_history,\n",
    "    top_item_links=top_item_links, \n",
    "    user_items=user_items,\n",
    "    batch_size=2000 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccfaa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] Khá»Ÿi táº¡o dá»¯ liá»‡u ná»n táº£ng...\n",
      " âœ… ÄÃ£ náº¡p Ground Truth: 391,900 users.\n",
      " âœ… ÄÃ£ táº¡o Candidate Pool (Top 200).\n",
      " âœ… ÄÃ£ táº¡o User-Item History: 628,069 users.\n",
      " âœ… XÃ¡c Ä‘á»‹nh: 628,069 Warm users vÃ  391,900 Cold users.\n",
      "[2/4] Äang tÃ­nh toÃ¡n Ma tráº­n Co-occurrence (Item-Item)...\n",
      "[3/4] Äang táº¡o gá»£i Ã½ cÃ¡ nhÃ¢n hÃ³a cho Cold Users...\n",
      "âœ… [4/4] HoÃ n táº¥t! Tá»•ng gá»£i Ã½ Cold Start: 3,917,010\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 6. COLD START HANDLING (FULL PIPELINE: GT + POPULAR + CO-OCCURRENCE)\n",
    "# ======================================================================\n",
    "import polars as pl\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(\"[1/4] Khá»Ÿi táº¡o dá»¯ liá»‡u ná»n táº£ng...\")\n",
    "\n",
    "# --- 1. Náº¡p Ground Truth (.pkl) ---\n",
    "gt_path = \"groundtruth_main.pkl\"\n",
    "if os.path.exists(gt_path):\n",
    "    gt_raw = joblib.load(gt_path)\n",
    "    print(f\" âœ… ÄÃ£ náº¡p Ground Truth: {len(gt_raw):,} users.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"âŒ KhÃ´ng tÃ¬m tháº¥y file {gt_path}!\")\n",
    "\n",
    "# --- 2. Táº¡o danh sÃ¡ch Popular Items (Candidate Pool) ---\n",
    "# Giáº£i quyáº¿t lá»—i NameError: 'popular_items'\n",
    "popular_items = (\n",
    "    full_history\n",
    "    .group_by(\"item_id\")\n",
    "    .agg(pl.len().alias(\"cnt\"))\n",
    "    .sort(\"cnt\", descending=True)\n",
    "    .head(200)\n",
    "    .select(pl.col(\"item_id\").alias(\"X_0\"))\n",
    "    .collect()\n",
    ")\n",
    "popular_items_list = [int(i) for i in popular_items.get_column(\"X_0\").to_list()[:10]]\n",
    "print(f\" âœ… ÄÃ£ táº¡o Candidate Pool (Top 200).\")\n",
    "\n",
    "# --- 3. Táº¡o báº£ng lá»‹ch sá»­ mua hÃ ng (User-Item History) ---\n",
    "# Giáº£i quyáº¿t lá»—i NameError: 'user_items'\n",
    "user_items = (\n",
    "    full_history\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").cast(pl.Int64), \n",
    "        pl.col(\"item_id\").cast(pl.Int64)\n",
    "    ])\n",
    "    .drop_nulls()\n",
    "    .unique() \n",
    "    .group_by(\"customer_id\")\n",
    "    .agg(pl.col(\"item_id\").alias(\"item_id_list\"))\n",
    "    .collect()\n",
    ")\n",
    "print(f\" âœ… ÄÃ£ táº¡o User-Item History: {user_items.height:,} users.\")\n",
    "\n",
    "# --- 4. XÃ¡c Ä‘á»‹nh nhÃ³m Cold Users ---\n",
    "target_users = set(gt_raw.keys()) \n",
    "warm_users_list = recommendations_warm.select(\"X_-1\").unique().get_column(\"X_-1\").to_list()\n",
    "warm_users_set = set(str(u) for u in warm_users_list)\n",
    "cold_users_ids = list(target_users - warm_users_set)\n",
    "print(f\" âœ… XÃ¡c Ä‘á»‹nh: {len(warm_users_set):,} Warm users vÃ  {len(cold_users_ids):,} Cold users.\")\n",
    "\n",
    "# --- 5. TÃ­nh toÃ¡n Ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n (Co-occurrence Matrix) ---\n",
    "print(\"[2/4] Äang tÃ­nh toÃ¡n Ma tráº­n Co-occurrence (Item-Item)...\")\n",
    "user_items_clean = (\n",
    "    user_items\n",
    "    .explode(\"item_id_list\")\n",
    "    .rename({\"item_id_list\": \"item_id\"})\n",
    "    .cast({\"item_id\": pl.Int64})\n",
    ")\n",
    "\n",
    "item_pairs = (\n",
    "    user_items_clean.join(user_items_clean, on=\"customer_id\", suffix=\"_right\")\n",
    "    .filter(pl.col(\"item_id\") < pl.col(\"item_id_right\"))\n",
    "    .group_by([\"item_id\", \"item_id_right\"])\n",
    "    .len()\n",
    "    .filter(pl.col(\"len\") > 2)\n",
    ")\n",
    "\n",
    "top_item_links = (\n",
    "    pl.concat([\n",
    "        item_pairs.select([pl.col(\"item_id\"), pl.col(\"item_id_right\"), \"len\"]),\n",
    "        item_pairs.select([pl.col(\"item_id_right\").alias(\"item_id\"), pl.col(\"item_id\").alias(\"item_id_right\"), \"len\"])\n",
    "    ])\n",
    "    .sort([\"item_id\", \"len\"], descending=[False, True])\n",
    "    .group_by(\"item_id\")\n",
    "    .head(10)\n",
    "    .select([\n",
    "        pl.col(\"item_id\").alias(\"item_id_source\"), \n",
    "        pl.col(\"item_id_right\").alias(\"item_id_rec\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# --- 6. Sinh gá»£i Ã½ cho nhÃ³m Cold Start ---\n",
    "print(\"[3/4] Äang táº¡o gá»£i Ã½ cÃ¡ nhÃ¢n hÃ³a cho Cold Users...\")\n",
    "df_cold = pl.DataFrame({\"X_-1\": [int(u) for u in cold_users_ids]})\n",
    "\n",
    "# Logic Semi-cold (DÃ¹ng Co-occurrence)\n",
    "batch_semi = (\n",
    "    df_cold.join(user_items.rename({\"customer_id\": \"X_-1\"}), on=\"X_-1\", how=\"inner\")\n",
    "    .explode(\"item_id_list\")\n",
    "    .rename({\"item_id_list\": \"item_id\"})\n",
    "    .join(top_item_links, left_on=\"item_id\", right_on=\"item_id_source\", how=\"inner\")\n",
    "    .filter(pl.col(\"item_id_rec\") != pl.col(\"item_id\"))\n",
    "    .group_by([\"X_-1\", \"item_id_rec\"])\n",
    "    .len()\n",
    "    .sort([\"X_-1\", \"len\"], descending=[False, True])\n",
    "    .group_by(\"X_-1\")\n",
    "    .head(10)\n",
    "    .select([pl.col(\"X_-1\"), pl.col(\"item_id_rec\").alias(\"X_0\")])\n",
    ")\n",
    "\n",
    "# Logic Pure-cold (DÃ¹ng Popularity)\n",
    "batch_pure = (\n",
    "    df_cold.join(batch_semi.select(\"X_-1\").unique(), on=\"X_-1\", how=\"anti\")\n",
    "    .with_columns(pl.lit(popular_items_list).alias(\"X_0\"))\n",
    "    .explode(\"X_0\")\n",
    "    .select([\"X_-1\", \"X_0\"])\n",
    ")\n",
    "\n",
    "# --- 7. Káº¿t quáº£ cuá»‘i cÃ¹ng ---\n",
    "recommendations_cold = pl.concat([batch_semi, batch_pure]).unique()\n",
    "print(f\"âœ… [4/4] HoÃ n táº¥t! Tá»•ng gá»£i Ã½ Cold Start: {recommendations_cold.height:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8423dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ FINAL MEAN PRECISION@10: 0.022038\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 7. FINAL EVALUATION\n",
    "# ======================================================================\n",
    "# Ná»‘i báº£ng\n",
    "recommendations_all = pl.concat([\n",
    "    recommendations_warm.select([pl.col(\"X_-1\").cast(pl.Int64), pl.col(\"X_0\").cast(pl.Int64)]),\n",
    "    recommendations_cold.select([pl.col(\"X_-1\").cast(pl.Int64), pl.col(\"X_0\").cast(pl.Int64)])\n",
    "]).unique(subset=[\"X_-1\", \"X_0\"])\n",
    "gt = joblib.load(gt_path)\n",
    "\n",
    "# Chuyá»ƒn sang Dict Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm nhanh\n",
    "pred_df = recommendations_all.group_by(\"X_-1\").agg(pl.col(\"X_0\"))\n",
    "pred_data = pred_df.to_dict(as_series=False)\n",
    "pred_dict = dict(zip(pred_data[\"X_-1\"], pred_data[\"X_0\"]))\n",
    "gt_dict = {int(u): set(map(int, v)) for u, v in gt.items()}\n",
    "\n",
    "# TÃ­nh Precision@10\n",
    "precisions = []\n",
    "for u, gt_items in gt_dict.items():\n",
    "    preds = pred_dict.get(u, [])[:10]\n",
    "    correct = sum(1 for it in preds if it in gt_items)\n",
    "    precisions.append(correct / 10)\n",
    "\n",
    "print(f\"ğŸ¯ FINAL MEAN PRECISION@10: {np.mean(precisions):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
