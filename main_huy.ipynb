{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a923da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import joblib\n",
    "from src.data_loader import get_all_data, split_date_lazy\n",
    "from src.feature_engineering import build_features_from_purchases, build_labels\n",
    "from src.trainer import train_model\n",
    "from src import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ecbfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading and splitting data...\n",
      "üìÇ ƒêang qu√©t d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c: /home/genyonguyen/Data/CS116_Reccommender_System/data\n",
      "‚úÖ ƒê√£ load Items\n",
      "‚úÖ ƒê√£ load Users\n",
      "‚úÖ ƒê√£ load Purchases\n",
      "‚úÖ Data split completed.\n",
      "   - Hist samples: 29,580,965\n",
      "   - Recent samples: 3,099,667\n",
      "   - Val samples: 3,049,193\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 2. LOAD & SPLIT DATA USING CUSTOM UTILITY\n",
    "# ======================================================================\n",
    "print(\"üìÇ Loading and splitting data...\")\n",
    "\n",
    "# 1. Load d·ªØ li·ªáu\n",
    "lf_items, lf_users, lf_purchases = get_all_data(\"/home/genyonguyen/Data/CS116_Reccommender_System/data\")\n",
    "\n",
    "# 2. S·ª≠ d·ª•ng h√†m c·ªßa b·∫°n ƒë·ªÉ chia t·∫≠p\n",
    "# H√†m n√†y t·ª± ƒë·ªông x·ª≠ l√Ω cast datetime n·∫øu created_date ƒëang l√† String\n",
    "lf_purchase_hist, lf_purchase_recent, lf_purchase_val = split_date_lazy(\n",
    "    lf_purchases, \n",
    "    date_column_name=\"created_date\"\n",
    ")\n",
    "\n",
    "# 3. Ki·ªÉm tra nhanh k·∫øt qu·∫£ b·∫±ng collect_schema (kh√¥ng t·ªën RAM)\n",
    "print(\"‚úÖ Data split completed.\")\n",
    "print(f\"   - Hist samples: {lf_purchase_hist.select(pl.len()).collect().item():,}\")\n",
    "print(f\"   - Recent samples: {lf_purchase_recent.select(pl.len()).collect().item():,}\")\n",
    "print(f\"   - Val samples: {lf_purchase_val.select(pl.len()).collect().item():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08457b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë user trong Hist: 2,170,516\n",
      "S·ªë user trong Recent: 650,383\n",
      "S·ªë user giao thoa: 507,012\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra xem c√≥ bao nhi√™u user xu·∫•t hi·ªán ·ªü c·∫£ Hist v√† Recent\n",
    "user_hist = lf_purchase_hist.select(\"customer_id\").unique().collect()\n",
    "user_recent = lf_purchase_recent.select(\"customer_id\").unique().collect()\n",
    "\n",
    "intersection = user_hist.join(user_recent, on=\"customer_id\", how=\"inner\")\n",
    "print(f\"S·ªë user trong Hist: {user_hist.height:,}\")\n",
    "print(f\"S·ªë user trong Recent: {user_recent.height:,}\")\n",
    "print(f\"S·ªë user giao thoa: {intersection.height:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8961326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu n·ªÅn t·∫£ng cho Model v√† Inference...\n",
      " ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file groundtruth_main.pkl!\n",
      " üõ†Ô∏è ƒêang t√≠nh to√°n Ma tr·∫≠n Co-occurrence...\n",
      " üõ†Ô∏è ƒêang x√¢y d·ª±ng infer_features_df t·ª´ full_history...\n",
      "\n",
      "üöÄ KH·ªûI T·∫†O XONG! C√°c bi·∫øn ƒë√£ kh·ªõp Schema String/Int32.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 3. INITIALIZE CORE DATA & CO-OCCURRENCE (FIXED FOR SCHEMA)\n",
    "# ======================================================================\n",
    "import os\n",
    "import joblib\n",
    "import polars as pl\n",
    "\n",
    "print(\"üèóÔ∏è ƒêang chu·∫©n b·ªã d·ªØ li·ªáu n·ªÅn t·∫£ng cho Model v√† Inference...\")\n",
    "\n",
    "# 1. G·ªôp to√†n b·ªô l·ªãch s·ª≠ mua h√†ng \n",
    "# L∆ØU √ù: Ph·∫£i g·ªôp c·∫£ lf_purchase_hist n·∫øu mu·ªën t√≠nh X1-X3 ƒë·∫ßy ƒë·ªß nh·∫•t\n",
    "full_history = pl.concat([lf_purchase_val])\n",
    "\n",
    "# 2. N·∫°p Ground Truth\n",
    "gt_path = \"groundtruth_main.pkl\"\n",
    "if os.path.exists(gt_path):\n",
    "    gt_raw = joblib.load(gt_path)\n",
    "    target_users_set = set(str(u) for u in gt_raw.keys())\n",
    "    print(f\" ‚úÖ ƒê√£ n·∫°p Ground Truth: {len(target_users_set):,} users.\")\n",
    "else:\n",
    "    print(\" ‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file groundtruth_main.pkl!\")\n",
    "\n",
    "# 3. T·∫°o b·∫£ng l·ªãch s·ª≠ mua h√†ng r√∫t g·ªçn\n",
    "# S·ª¨A L·ªñI: item_id l√† String, customer_id l√† Int32 theo Schema c·ªßa b·∫°n\n",
    "user_items = (\n",
    "    full_history\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").cast(pl.Int32), # Kh·ªõp Schema Int32\n",
    "        pl.col(\"item_id\").cast(pl.String)     # Kh·ªõp Schema String\n",
    "    ])\n",
    "    .drop_nulls()\n",
    "    .unique() \n",
    "    .group_by(\"customer_id\")\n",
    "    .agg(pl.col(\"item_id\").alias(\"item_id_list\"))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# 4. T√≠nh to√°n Ma tr·∫≠n ƒë·ªìng xu·∫•t hi·ªán (top_item_links)\n",
    "print(\" üõ†Ô∏è ƒêang t√≠nh to√°n Ma tr·∫≠n Co-occurrence...\")\n",
    "user_items_clean = user_items.explode(\"item_id_list\").rename({\"item_id_list\": \"item_id\"})\n",
    "\n",
    "# Self-join ƒë·ªÉ t√¨m c√°c c·∫∑p s·∫£n ph·∫©m mua c√πng nhau\n",
    "item_pairs = (\n",
    "    user_items_clean.join(user_items_clean, on=\"customer_id\", suffix=\"_right\")\n",
    "    .filter(pl.col(\"item_id\") < pl.col(\"item_id_right\")) # Tr√°nh tr√πng l·∫∑p c·∫∑p (A,B) v√† (B,A)\n",
    "    .group_by([\"item_id\", \"item_id_right\"])\n",
    "    .len()\n",
    "    .filter(pl.col(\"len\") > 2) \n",
    ")\n",
    "\n",
    "top_item_links = (\n",
    "    pl.concat([\n",
    "        item_pairs.select([pl.col(\"item_id\"), pl.col(\"item_id_right\"), \"len\"]),\n",
    "        item_pairs.select([pl.col(\"item_id_right\").alias(\"item_id\"), pl.col(\"item_id\").alias(\"item_id_right\"), \"len\"])\n",
    "    ])\n",
    "    .sort([\"item_id\", \"len\"], descending=[False, True])\n",
    "    .group_by(\"item_id\")\n",
    "    .head(12) \n",
    "    .select([\n",
    "        pl.col(\"item_id\").alias(\"item_id_source\"), \n",
    "        pl.col(\"item_id_right\").alias(\"item_id_rec\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 5. T·∫°o ƒë·∫∑c tr∆∞ng ph·ª•c v·ª• Inference (infer_features_df)\n",
    "print(\" üõ†Ô∏è ƒêang x√¢y d·ª±ng infer_features_df t·ª´ full_history...\")\n",
    "# H√†m n√†y s·∫Ω t·ª± ƒë·ªông nh·∫≠n di·ªán item_id l√† String v√† customer_id l√† Int32\n",
    "infer_features_df = build_features_from_purchases(\n",
    "    lf_purchases=full_history, \n",
    "    lf_items=lf_items, \n",
    "    lf_users=lf_users\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ KH·ªûI T·∫†O XONG! C√°c bi·∫øn ƒë√£ kh·ªõp Schema String/Int32.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e98904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ† Building features from Hist...\n",
      "üéØ Building labels from Recent...\n",
      "üöÄ Building Labels with Vectorized Hard Negative Strategy...\n",
      "üîó Merging via Entity Profile...\n",
      "‚úÖ Train dataset created: 18,765,984 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>X_-1</th><th>X_0</th><th>X_1</th><th>X_2</th><th>X_3</th><th>Y</th></tr><tr><td>i32</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>i8</td></tr></thead><tbody><tr><td>5624205</td><td>&quot;0007160000078&quot;</td><td>0.0</td><td>99.0</td><td>0.0</td><td>1</td></tr><tr><td>6642730</td><td>&quot;5481000000002&quot;</td><td>0.0</td><td>63.0</td><td>0.0</td><td>1</td></tr><tr><td>5777942</td><td>&quot;6849000000003&quot;</td><td>3.0</td><td>21.0</td><td>3.0</td><td>1</td></tr><tr><td>6288770</td><td>&quot;5952000000001&quot;</td><td>0.0</td><td>41.0</td><td>0.0</td><td>1</td></tr><tr><td>5415343</td><td>&quot;0952000000074&quot;</td><td>1.0</td><td>8.0</td><td>0.0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ X_-1    ‚îÜ X_0           ‚îÜ X_1 ‚îÜ X_2  ‚îÜ X_3 ‚îÜ Y   ‚îÇ\n",
       "‚îÇ ---     ‚îÜ ---           ‚îÜ --- ‚îÜ ---  ‚îÜ --- ‚îÜ --- ‚îÇ\n",
       "‚îÇ i32     ‚îÜ str           ‚îÜ f64 ‚îÜ f64  ‚îÜ f64 ‚îÜ i8  ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 5624205 ‚îÜ 0007160000078 ‚îÜ 0.0 ‚îÜ 99.0 ‚îÜ 0.0 ‚îÜ 1   ‚îÇ\n",
       "‚îÇ 6642730 ‚îÜ 5481000000002 ‚îÜ 0.0 ‚îÜ 63.0 ‚îÜ 0.0 ‚îÜ 1   ‚îÇ\n",
       "‚îÇ 5777942 ‚îÜ 6849000000003 ‚îÜ 3.0 ‚îÜ 21.0 ‚îÜ 3.0 ‚îÜ 1   ‚îÇ\n",
       "‚îÇ 6288770 ‚îÜ 5952000000001 ‚îÜ 0.0 ‚îÜ 41.0 ‚îÜ 0.0 ‚îÜ 1   ‚îÇ\n",
       "‚îÇ 5415343 ‚îÜ 0952000000074 ‚îÜ 1.0 ‚îÜ 8.0  ‚îÜ 0.0 ‚îÜ 1   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 4. BUILDING TRAIN DATASET (CHU·∫®N H√ìA THEO SCHEMA)\n",
    "# ======================================================================\n",
    "\n",
    "def build_features_from_purchases_fixed(lf_purchases, lf_items, lf_users=None):\n",
    "    # 1. JOIN metadata - Ch√∫ √Ω: item_id ƒë·ªÅu l√† String n√™n Join r·∫•t an to√†n\n",
    "    data_lf = lf_purchases.join(\n",
    "        lf_items.select(['item_id', 'brand', 'age_group', 'category']),\n",
    "        on='item_id',\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # 2. T√çNH TO√ÅN V√Ä GI·ªÆ L·∫†I C·ªòT KEY\n",
    "    feature_df = (\n",
    "        data_lf\n",
    "        .select([\n",
    "            pl.col(\"customer_id\").alias(\"X_-1\"), # Gi·ªØ Int32 theo Schema\n",
    "            pl.col(\"item_id\").alias(\"X_0\"),      # Gi·ªØ String theo Schema\n",
    "            pl.col(\"brand\"),                     # String\n",
    "            pl.col(\"category\"),                  # String\n",
    "            pl.col(\"age_group\"),                 # String\n",
    "\n",
    "            # T√≠nh to√°n t·∫ßn su·∫•t\n",
    "            pl.len().over([\"customer_id\", \"brand\"]).alias(\"X_1\"),\n",
    "            pl.len().over([\"customer_id\", \"age_group\"]).alias(\"X_2\"),\n",
    "            pl.len().over([\"customer_id\", \"category\"]).alias(\"X_3\"),\n",
    "        ])\n",
    "        .unique(subset=[\"X_-1\", \"X_0\"])\n",
    "        .with_columns([\n",
    "            pl.col(\"X_1\").cast(pl.Float64),\n",
    "            pl.col(\"X_2\").cast(pl.Float64),\n",
    "            pl.col(\"X_3\").cast(pl.Float64),\n",
    "        ])\n",
    "    )\n",
    "    return feature_df\n",
    "\n",
    "print(\"üõ† Building features from Hist...\")\n",
    "train_features_df = build_features_from_purchases_fixed(lf_purchase_hist, lf_items)\n",
    "\n",
    "print(\"üéØ Building labels from Recent...\")\n",
    "train_label_df = build_labels(lf_purchase_hist, lf_purchase_recent, lf_items, negative_ratio=1.0)\n",
    "\n",
    "print(\"üîó Merging via Entity Profile...\")\n",
    "# Ch√∫ √Ω: L·∫•y Metadata g·ªëc (String) ƒë·ªÉ Join\n",
    "train_label_with_meta = train_label_df.join(\n",
    "    lf_items.select([\"item_id\", \"brand\", \"category\", \"age_group\"]),\n",
    "    on=\"item_id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# B∆∞·ªõc 4: T√°ch profile (Kh√¥ng cast Int64 cho brand/category n·ªØa)\n",
    "user_brand_feat = train_features_df.select([\"X_-1\", \"brand\", \"X_1\"]).unique(subset=[\"X_-1\", \"brand\"])\n",
    "user_age_feat   = train_features_df.select([\"X_-1\", \"age_group\", \"X_2\"]).unique(subset=[\"X_-1\", \"age_group\"])\n",
    "user_cat_feat   = train_features_df.select([\"X_-1\", \"category\", \"X_3\"]).unique(subset=[\"X_-1\", \"category\"])\n",
    "\n",
    "# B∆∞·ªõc 5: Join t·ªïng h·ª£p\n",
    "train_df = (\n",
    "    train_label_with_meta\n",
    "    .select([\n",
    "        pl.col(\"customer_id\").alias(\"X_-1\"),\n",
    "        pl.col(\"item_id\").alias(\"X_0\"),\n",
    "        pl.col(\"label\").alias(\"Y\"),\n",
    "        \"brand\", \"category\", \"age_group\"\n",
    "    ])\n",
    "    .join(user_brand_feat, on=[\"X_-1\", \"brand\"], how=\"left\")\n",
    "    .join(user_age_feat, on=[\"X_-1\", \"age_group\"], how=\"left\")\n",
    "    .join(user_cat_feat, on=[\"X_-1\", \"category\"], how=\"left\")\n",
    "    .with_columns(\n",
    "        pl.col([\"X_1\", \"X_2\", \"X_3\"]).fill_null(0)\n",
    "    )\n",
    "    .select([\"X_-1\", \"X_0\", \"X_1\", \"X_2\", \"X_3\", \"Y\"])\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train dataset created: {train_df.select(pl.len()).collect().item():,} rows.\")\n",
    "display(train_df.limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308ceba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TH·ªêNG K√ä TRAIN_DF (T·ªïng s·ªë d√≤ng: 18,766,176)\n",
      "---------------------------------------------\n",
      "üö´ S·ªë d√≤ng c√≥ X1, X2, X3 ƒë·ªÅu b·∫±ng 0: 695,393 (3.71%)\n",
      "üî∏ C·ªôt X_1 (Brand) b·∫±ng 0: 4,761,292 (25.37%)\n",
      "üî∏ C·ªôt X_2 (Age Group) b·∫±ng 0: 4,276,069 (22.79%)\n",
      "üî∏ C·ªôt X_3 (Category) b·∫±ng 0: 1,698,387 (9.05%)\n",
      "---------------------------------------------\n",
      "‚úÖ T·ª∑ l·ªá d·ªØ li·ªáu ·ªïn ƒë·ªãnh. B·∫°n c√≥ th·ªÉ ti·∫øn h√†nh hu·∫•n luy·ªán.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CHECK DATA DENSITY (KI·ªÇM TRA S·ªê D√íNG B·∫∞NG 0)\n",
    "# ======================================================================\n",
    "\n",
    "# T√≠nh to√°n th·ªëng k√™\n",
    "check_df = train_df.select([\n",
    "    pl.len().alias(\"total_rows\"),\n",
    "    # ƒê·∫øm s·ªë d√≤ng m√† c·∫£ 3 ƒë·∫∑c tr∆∞ng ƒë·ªÅu b·∫±ng 0\n",
    "    pl.struct([\"X_1\", \"X_2\", \"X_3\"])\n",
    "      .filter((pl.col(\"X_1\") == 0) & (pl.col(\"X_2\") == 0) & (pl.col(\"X_3\") == 0))\n",
    "      .count().alias(\"all_zeros\"),\n",
    "    \n",
    "    # ƒê·∫øm ri√™ng l·∫ª t·ª´ng c·ªôt\n",
    "    (pl.col(\"X_1\") == 0).sum().alias(\"X_1_zero\"),\n",
    "    (pl.col(\"X_2\") == 0).sum().alias(\"X_2_zero\"),\n",
    "    (pl.col(\"X_3\") == 0).sum().alias(\"X_3_zero\")\n",
    "]).collect()\n",
    "\n",
    "# Tr√≠ch xu·∫•t gi√° tr·ªã\n",
    "total = check_df[\"total_rows\"][0]\n",
    "all_zero = check_df[\"all_zeros\"][0]\n",
    "x1_z = check_df[\"X_1_zero\"][0]\n",
    "x2_z = check_df[\"X_2_zero\"][0]\n",
    "x3_z = check_df[\"X_3_zero\"][0]\n",
    "\n",
    "print(f\"üìä TH·ªêNG K√ä TRAIN_DF (T·ªïng s·ªë d√≤ng: {total:,})\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"üö´ S·ªë d√≤ng c√≥ X1, X2, X3 ƒë·ªÅu b·∫±ng 0: {all_zero:,} ({all_zero/total*100:.2f}%)\")\n",
    "print(f\"üî∏ C·ªôt X_1 (Brand) b·∫±ng 0: {x1_z:,} ({x1_z/total*100:.2f}%)\")\n",
    "print(f\"üî∏ C·ªôt X_2 (Age Group) b·∫±ng 0: {x2_z:,} ({x2_z/total*100:.2f}%)\")\n",
    "print(f\"üî∏ C·ªôt X_3 (Category) b·∫±ng 0: {x3_z:,} ({x3_z/total*100:.2f}%)\") \n",
    "print(\"-\" * 45)\n",
    "\n",
    "if all_zero / total > 0.8:\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: T·ª∑ l·ªá d√≤ng tr·ªëng qu√° cao! H√£y ki·ªÉm tra l·∫°i logic t·∫°o Candidate.\")\n",
    "else:\n",
    "    print(\"‚úÖ T·ª∑ l·ªá d·ªØ li·ªáu ·ªïn ƒë·ªãnh. B·∫°n c√≥ th·ªÉ ti·∫øn h√†nh hu·∫•n luy·ªán.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5237537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# 5. TRAINING MODEL (OPTIMIZED FOR X1, X2, X3 & SCHEMA)\n",
    "# ======================================================================\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "import polars as pl\n",
    "from src import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42751cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = joblib.load(config.MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6f689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Collecting and processing training data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m feature_cols = [\u001b[33m\"\u001b[39m\u001b[33mX_1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mX_2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mX_3\u001b[39m\u001b[33m\"\u001b[39m] \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Th·ª±c thi hu·∫•n luy·ªán\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model_xgb = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 4. L∆∞u m√¥ h√¨nh\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data/Rcm_System/src/trainer.py:17\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_df, feature_cols)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚è≥ Collecting and processing training data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 1. Chuy·ªÉn ƒë·ªïi an to√†n: \u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# √âp ki·ªÉu features v·ªÅ Float32 v√† ƒë·∫£m b·∫£o nh√£n Y l√† s·ªë nguy√™n cho XGBoost\u001b[39;00m\n\u001b[32m     10\u001b[39m train_pd = (\n\u001b[32m     11\u001b[39m     \u001b[43mtrain_df\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ ti·∫øt ki·ªám RAM\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_null\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInt8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Nh√£n 0/1 ch·ªâ c·∫ßn Int8\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     .to_pandas()\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m X_train = train_pd[feature_cols]\n\u001b[32m     22\u001b[39m y_train = train_pd[\u001b[33m\"\u001b[39m\u001b[33mY\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/polars/_utils/deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/polars/lazyframe/opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/polars/lazyframe/frame.py:2415\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2413\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2414\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Danh s√°ch feature b·∫°n ƒë√£ ch·ªçn (X1: Brand, X2: Age, X3: Cat)\n",
    "feature_cols = [\"X_1\", \"X_2\", \"X_3\"] \n",
    "\n",
    "# Th·ª±c thi hu·∫•n luy·ªán\n",
    "model_xgb = train_model(train_df, feature_cols)\n",
    "\n",
    "# 4. L∆∞u m√¥ h√¨nh\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "joblib.dump(model_xgb, config.MODEL_PATH)\n",
    "print(f\"‚úÖ Model trained and saved at: {config.MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77bf0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä T·∫¶M QUAN TR·ªåNG C·ª¶A C√ÅC T√çNH NƒÇNG (FEATURE IMPORTANCE):\n",
      "  - X_3: 4404.66\n",
      "  - X_1: 873.54\n",
      "  - X_2: 678.32\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# L·∫•y ƒëi·ªÉm quan tr·ªçng c·ªßa c√°c t√≠nh nƒÉng\n",
    "importance = model_xgb.get_score(importance_type='gain') # Ho·∫∑c 'weight'\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Hi·ªÉn th·ªã\n",
    "print(\"üìä T·∫¶M QUAN TR·ªåNG C·ª¶A C√ÅC T√çNH NƒÇNG (FEATURE IMPORTANCE):\")\n",
    "for feat, score in sorted_importance:\n",
    "    print(f\"  - {feat}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279271fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "gt = pkl.load(open(\"groundtruth.pkl\", \"rb\"))\n",
    "target_users_set = set(gt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a022ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_users_set = set(\n",
    "    lf_purchase_val\n",
    "    .select(\"customer_id\")\n",
    "    .unique()\n",
    "    .collect(engine=\"streaming\")\n",
    "    .to_series()\n",
    "    .to_list()\n",
    ") & target_users_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3753f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "df_purchase_val = (\n",
    "    lf_purchase_val\n",
    "    .select([\"customer_id\", \"item_id\"])\n",
    "    .join(lf_items.select([\"item_id\", \"brand\", \"age_group\", \"category\"]), on=\"item_id\", how=\"left\")\n",
    "    .collect(engine=\"streaming\")\n",
    ")\n",
    "\n",
    "# Aggregate counts for each attribute in parallel\n",
    "brand_counts = (\n",
    "    df_purchase_val.group_by(['customer_id', 'brand'])\n",
    "    .agg(pl.len().alias('count'))\n",
    ")\n",
    "\n",
    "age_counts = (\n",
    "    df_purchase_val.group_by(['customer_id', 'age_group'])\n",
    "    .agg(pl.len().alias('count'))\n",
    ")\n",
    "\n",
    "category_counts = (\n",
    "    df_purchase_val.group_by(['customer_id', 'category'])\n",
    "    .agg(pl.len().alias('count'))\n",
    ")\n",
    "\n",
    "# Build the nested defaultdict structure\n",
    "feature_dict = defaultdict(lambda: {\n",
    "    'brand': {},\n",
    "    'age_group': {},\n",
    "    'category': {}\n",
    "})\n",
    "\n",
    "# Populate brand counts\n",
    "for row in brand_counts.iter_rows(named=True):\n",
    "    feature_dict[row['customer_id']]['brand'][row['brand']] = row['count']\n",
    "\n",
    "# Populate age_group counts\n",
    "for row in age_counts.iter_rows(named=True):\n",
    "    feature_dict[row['customer_id']]['age_group'][row['age_group']] = row['count']\n",
    "\n",
    "# Populate category counts\n",
    "for row in category_counts.iter_rows(named=True):\n",
    "    feature_dict[row['customer_id']]['category'][row['category']] = row['count']\n",
    "\n",
    "del df_purchase_val\n",
    "del brand_counts\n",
    "del age_counts\n",
    "del category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fa1176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_dict = (\n",
    "    lf_items\n",
    "    .with_columns(\n",
    "        pl.concat_str([\"brand\", \"age_group\", \"category\"], separator=\"|\")\n",
    "        .str.split(\"|\")\n",
    "        .alias(\"values\")\n",
    "    )\n",
    "    .select([\"item_id\", \"values\"])\n",
    "    .collect()\n",
    "    .to_dict(as_series=False)\n",
    ")\n",
    "\n",
    "item_feature_dict = dict(zip(item_feature_dict[\"item_id\"], item_feature_dict[\"values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b88e7395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Build lookup structures] Completed in 2.89 seconds\n",
      "[Build item co-purchase matrix] Completed in 2.55 seconds\n",
      "[Compute item popularity] Completed in 0.59 seconds\n",
      "[Build item similarity map] Completed in 7.54 seconds\n",
      "[Collect item features] Completed in 0.07 seconds\n",
      "[Collect user purchases with features] Completed in 0.57 seconds\n",
      "[Cache business rule candidates] Completed in 0.58 seconds\n",
      "[Build user reorder candidates] Completed in 0.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "664277it [00:00, 1105148.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Timer was not started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 244990/244990 [2:00:48<00:00, 33.80it/s]  \n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 6. RUN INFERENCE FOR WARM USERS (OPTIMIZED WITH ABSOLUTE BOOSTING)\n",
    "# ======================================================================\n",
    "import xgboost as xgb\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from src.candidate_selector import CandidateSelector\n",
    "from tqdm import tqdm\n",
    "\n",
    "customer_ids = []\n",
    "pred_ids = []\n",
    "\n",
    "purchases_recent_val_lf = pl.concat([lf_purchase_recent, lf_purchase_val])\n",
    "cs = CandidateSelector(purchases_recent_val_lf, lf_users, lf_items)\n",
    "\n",
    "# --- B∆Ø·ªöC 3: D·ª∞ ƒêO√ÅN V√Ä RERANK THEO BATCH ---\n",
    "for customer_id in tqdm(warm_users_set):\n",
    "\n",
    "    candidates = cs.get_candidates(customer_id)\n",
    "\n",
    "    # Feature extract\n",
    "    features = []\n",
    "    for c_id in candidates:\n",
    "        brand_count = feature_dict[customer_id][\"brand\"].get(item_feature_dict[c_id][0], 0)\n",
    "        age_count = feature_dict[customer_id][\"age_group\"].get(item_feature_dict[c_id][1], 0)\n",
    "        category_count = feature_dict[customer_id][\"category\"].get(item_feature_dict[c_id][2], 0)\n",
    "        features.append([brand_count, age_count, category_count])\n",
    "\n",
    "    X_infer = pl.DataFrame(features, schema=[\"X_1\", \"X_2\", \"X_3\"], orient='row')\n",
    "    probs = model_xgb.predict(xgb.DMatrix(X_infer))\n",
    "\n",
    "    ranked_indices = np.argsort(probs)[::-1]\n",
    "    ranked_results = []\n",
    "    \n",
    "    # Rerank\n",
    "    for idx in ranked_indices[:10]:\n",
    "        ranked_results.append(candidates[idx])\n",
    "    \n",
    "    customer_ids.extend([customer_id] * len(ranked_results))\n",
    "    pred_ids.extend(ranked_results)\n",
    "\n",
    "\n",
    "recommendations_warm = pl.DataFrame({\"X_-1\": customer_ids, \"X_0\": pred_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ecd05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "recommendations_warm = json.load(open(\"recommendations_warm.json\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a104288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_warm = {int(k): list(map(str, v)) for k, v in recommendations_warm.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044d87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = (\n",
    "    lf_purchase_val.select([\"customer_id\", \"item_id\"])\n",
    "    .group_by(\"customer_id\")\n",
    "    .agg(pl.col(\"item_id\"))\n",
    "    .collect()\n",
    "    .to_dict(as_series=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fcfb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = dict(zip(hist_dict[\"customer_id\"], hist_dict[\"item_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcdbe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "gt = pkl.load(open(\"groundtruth.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b44b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def precision_at_k_fast(pred, gt, hist, filter_bought_items=True, K=10):\n",
    "    precisions = []\n",
    "    cold_start_users = []\n",
    "\n",
    "    # users that have both prediction and history\n",
    "    valid_users = gt.keys() & pred.keys() & hist.keys()\n",
    "\n",
    "    skipped = 0\n",
    "    for user in tqdm(gt.keys()):\n",
    "        if user not in valid_users:\n",
    "            cold_start_users.append(user)\n",
    "            continue\n",
    "\n",
    "        # ground truth as set\n",
    "        relevant_items = set(gt[user])\n",
    "\n",
    "        if filter_bought_items:\n",
    "            relevant_items.difference_update(hist[user])\n",
    "\n",
    "        if not relevant_items:\n",
    "            skipped += 1\n",
    "            precisions.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # prediction@K\n",
    "        pred_topk = pred[user][:K]\n",
    "        hits = 0\n",
    "        for item in pred_topk:\n",
    "            if item in relevant_items:\n",
    "                hits += 1\n",
    "\n",
    "        precisions.append(hits / K)\n",
    "    \n",
    "    print(f\"Skipped: {skipped}\")\n",
    "\n",
    "    return np.mean(precisions), cold_start_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25776810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391900/391900 [00:00<00:00, 711886.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: 47470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.004439773051961304)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, _ = precision_at_k_fast(recommendations_warm, gt, hist_dict)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b3dde42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391900/391900 [00:00<00:00, 1053644.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.004439773051961304)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, _ = precision_at_k_fast(recommendations_warm, gt, hist_dict, filter_bought_items=False)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7032002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ ƒêang chu·∫©n b·ªã danh s√°ch Popular Items t·ª´ d·ªØ li·ªáu g·∫ßn nh·∫•t...\n",
      "‚úÖ ƒê√£ t·∫°o xong danh s√°ch 100 m√≥n ph·ªï bi·∫øn nh·∫•t.\n",
      "üìç V√≠ d·ª• 5 m√≥n ƒë·∫ßu b·∫£ng: ['4690000000001', '1512000000004', '2803000000013', '6768000000005', '0020020000185']\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 5. CREATE POPULAR ITEMS LIST (FOR COLD START STRATEGY)\n",
    "# ======================================================================\n",
    "\n",
    "print(\"üì¶ ƒêang chu·∫©n b·ªã danh s√°ch Popular Items t·ª´ d·ªØ li·ªáu g·∫ßn nh·∫•t...\")\n",
    "\n",
    "# N√™n s·ª≠ d·ª•ng d·ªØ li·ªáu c·ªßa th√°ng g·∫ßn nh·∫•t (lf_purchase_val) ƒë·ªÉ l·∫•y xu h∆∞·ªõng m·ªõi nh·∫•t\n",
    "# ƒêi·ªÅu n√†y gi√∫p tƒÉng Precision cho nh√≥m Cold l√™n m·ª©c ~0.011668\n",
    "popular_items_list = (\n",
    "    lf_purchase_val.group_by(\"item_id\")\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    "    .head(100) # L·∫•y 100 m√≥n ƒë·ªÉ l√†m kho d·ª± ph√≤ng (Candidate Pool)\n",
    "    .collect()\n",
    "    .get_column(\"item_id\")\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "# √âp ki·ªÉu sang String ƒë·ªÉ ƒë·ªìng nh·∫•t v·ªõi schema c·ªßa Submission v√† Item-Links\n",
    "popular_items_list_str = [str(i) for i in popular_items_list]\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫°o xong danh s√°ch {len(popular_items_list_str)} m√≥n ph·ªï bi·∫øn nh·∫•t.\")\n",
    "print(f\"üìç V√≠ d·ª• 5 m√≥n ƒë·∫ßu b·∫£ng: {popular_items_list_str[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1df597fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Ph√¢n t√°ch nh√≥m Warm v√† Cold Users...\n",
      " ‚úÖ Th·ªëng k√™: 244,990 Warm users v√† 146,910 Cold users.\n",
      "[2/3] ƒêang t·∫°o g·ª£i √Ω b·ªï tr·ª£ (Repurchase & Item-Links)...\n",
      "[3/3] ƒêang g·ªôp k·∫øt qu·∫£ v√† l·ªçc theo ƒë√∫ng danh s√°ch m·ª•c ti√™u...\n",
      " ‚úÖ T·∫§T C·∫¢ HO√ÄN T·∫§T!\n",
      " üìä T·ªïng s·ªë User th·ª±c t·∫ø trong file: 391,900\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# 7. HYBRID HANDLING (REPURCHASE + ITEM-LINKS + POPULAR) - FIXED VERSION\n",
    "# ======================================================================\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "print(\"[1/3] Ph√¢n t√°ch nh√≥m Warm v√† Cold Users...\")\n",
    "\n",
    "# 1. Thu th·∫≠p k·∫øt qu·∫£ t·ª´ Cell 6\n",
    "if isinstance(recommendations_warm, pl.LazyFrame):\n",
    "    warm_results = recommendations_warm.select([\n",
    "        pl.col(\"X_-1\").cast(pl.String),\n",
    "        pl.col(\"X_0\").cast(pl.String)\n",
    "    ]).collect()\n",
    "else:\n",
    "    warm_results = recommendations_warm.select([\n",
    "        pl.col(\"X_-1\").cast(pl.String),\n",
    "        pl.col(\"X_0\").cast(pl.String)\n",
    "    ])\n",
    "\n",
    "cold_users_ids = list(target_users_set - warm_users_set)\n",
    "print(f\" ‚úÖ Th·ªëng k√™: {len(warm_users_set & target_users_set):,} Warm users v√† {len(cold_users_ids):,} Cold users.\")\n",
    "\n",
    "# --- 2. Sinh g·ª£i √Ω b·ªï tr·ª£ ƒëa t·∫ßng ---\n",
    "print(\"[2/3] ƒêang t·∫°o g·ª£i √Ω b·ªï tr·ª£ (Repurchase & Item-Links)...\")\n",
    "df_cold_base = pl.DataFrame({\"X_-1\": [str(u) for u in cold_users_ids]}).lazy()\n",
    "\n",
    "# Nh√°nh A: Repurchase (ƒê·ªì c≈© cho 47k users)\n",
    "batch_repur = (\n",
    "    df_cold_base.join(\n",
    "        user_items.lazy().with_columns(pl.col(\"customer_id\").cast(pl.String)).rename({\"customer_id\": \"X_-1\"}),\n",
    "        on=\"X_-1\", how=\"inner\"\n",
    "    )\n",
    "    .explode(\"item_id_list\")\n",
    "    .select([pl.col(\"X_-1\"), pl.col(\"item_id_list\").alias(\"X_0\")])\n",
    ")\n",
    "\n",
    "# Nh√°nh B: Semi-cold (ƒê·ªì mua k√®m)\n",
    "batch_semi = (\n",
    "    df_cold_base.join(\n",
    "        user_items.lazy().with_columns(pl.col(\"customer_id\").cast(pl.String)).rename({\"customer_id\": \"X_-1\"}), \n",
    "        on=\"X_-1\", how=\"inner\"\n",
    "    )\n",
    "    .explode(\"item_id_list\")\n",
    "    .rename({\"item_id_list\": \"item_id\"})\n",
    "    .join(\n",
    "        top_item_links.lazy().with_columns([\n",
    "            pl.col(\"item_id_source\").cast(pl.String),\n",
    "            pl.col(\"item_id_rec\").cast(pl.String)\n",
    "        ]), \n",
    "        left_on=\"item_id\", right_on=\"item_id_source\", how=\"inner\"\n",
    "    )\n",
    "    .select([pl.col(\"X_-1\"), pl.col(\"item_id_rec\").alias(\"X_0\")])\n",
    ")\n",
    "\n",
    "# Nh√°nh C: Pure-cold (Popular)\n",
    "users_with_any_rec = pl.concat([\n",
    "    batch_semi.select(\"X_-1\").unique(),\n",
    "    batch_repur.select(\"X_-1\").unique()\n",
    "]).unique()\n",
    "\n",
    "batch_pure = (\n",
    "    df_cold_base.join(users_with_any_rec, on=\"X_-1\", how=\"anti\")\n",
    "    .with_columns(pl.lit(popular_items_list_str).alias(\"X_0\"))\n",
    "    .explode(\"X_0\")\n",
    "    .select([\"X_-1\", \"X_0\"])\n",
    ")\n",
    "\n",
    "# G·ªôp k·∫øt qu·∫£ Cold Start v·ªõi th·ª© t·ª± ∆∞u ti√™n\n",
    "recommendations_cold = pl.concat([\n",
    "    batch_repur.with_columns(pl.lit(1).alias(\"sub_prio\")),\n",
    "    batch_semi.with_columns(pl.lit(2).alias(\"sub_prio\")),\n",
    "    batch_pure.with_columns(pl.lit(3).alias(\"sub_prio\"))\n",
    "]).unique(subset=[\"X_-1\", \"X_0\"]).sort([\"X_-1\", \"sub_prio\"]).collect()\n",
    "\n",
    "# --- 3. K·∫æT H·ª¢P V√Ä XU·∫§T SUBMISSION (√âP THEO GT_DICT) ---\n",
    "print(\"[3/3] ƒêang g·ªôp k·∫øt qu·∫£ v√† l·ªçc theo ƒë√∫ng danh s√°ch m·ª•c ti√™u...\")\n",
    "\n",
    "# L·∫•y danh s√°ch ID chu·∫©n t·ª´ Ground Truth (ƒê√°p √°n c·ªßa th·∫ßy)\n",
    "final_target_ids = list(map(str, list(gt.keys())))\n",
    "\n",
    "submission = (\n",
    "    pl.concat([\n",
    "        warm_results.with_columns(pl.lit(1).alias(\"prio\")),\n",
    "        recommendations_cold.select([\"X_-1\", \"X_0\"]).with_columns(pl.lit(2).alias(\"prio\"))\n",
    "    ])\n",
    "    .cast({\"X_-1\": pl.String})\n",
    "    # B∆Ø·ªöC QUAN TR·ªåNG: Ch·ªâ gi·ªØ l·∫°i nh·ªØng ng∆∞·ªùi c√≥ trong ƒë√°p √°n th·ª±c t·∫ø\n",
    "    .filter(pl.col(\"X_-1\").is_in(final_target_ids)) \n",
    "    .unique(subset=[\"X_-1\", \"X_0\"]) \n",
    "    .sort([\"X_-1\", \"prio\"]) \n",
    "    .group_by(\"X_-1\")\n",
    "    .agg(pl.col(\"X_0\").head(12)) \n",
    "    .with_columns(\n",
    "        pl.col(\"X_0\").list.join(\" \").alias(\"prediction\")\n",
    "    )\n",
    "    .select([\n",
    "        pl.col(\"X_-1\").alias(\"customer_id\"),\n",
    "        \"prediction\"\n",
    "    ])\n",
    "    .unique(subset=[\"customer_id\"]) \n",
    ")\n",
    "\n",
    "# Ghi file\n",
    "submission.write_csv(\"final_submission.csv\")\n",
    "\n",
    "print(f\" ‚úÖ T·∫§T C·∫¢ HO√ÄN T·∫§T!\")\n",
    "print(f\" üìä T·ªïng s·ªë User th·ª±c t·∫ø trong file: {submission.height:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c2fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ƒêang n·∫°p Ground Truth t·ª´ file groundtruth_main.pkl...\n",
      "‚úÖ ƒê√£ n·∫°p xong 391,900 users trong Ground Truth.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# N·∫°p file pkl ch·ª©a ƒë√°p √°n th·ª±c t·∫ø\n",
    "print(\"üìÇ ƒêang n·∫°p Ground Truth t·ª´ file groundtruth_main.pkl...\")\n",
    "gt_dict_raw = pkl.load(open(\"groundtruth.pkl\", \"rb\"))\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi key sang String ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªõp v·ªõi customer_id trong submission\n",
    "gt_dict = {str(k): v for k, v in gt_dict_raw.items()}\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ n·∫°p xong {len(gt_dict):,} users trong Ground Truth.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b3946f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ chu·∫©n b·ªã pred_dict cho 391,900 users.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# 1. ƒê·ªçc l·∫°i file k·∫øt qu·∫£ v·ª´a xu·∫•t ra\n",
    "df_pred = pl.read_csv(\"final_submission.csv\")\n",
    "\n",
    "# 2. Chuy·ªÉn ƒë·ªïi t·ª´ b·∫£ng sang Dictionary ƒë·ªÉ tra c·ª©u nhanh trong h√†m ƒë√°nh gi√°\n",
    "# Format: { \"customer_id\": [\"item1\", \"item2\", ... , \"item12\"] }\n",
    "pred_dict = {\n",
    "    str(row[\"customer_id\"]): row[\"prediction\"].split(\" \") \n",
    "    for row in df_pred.iter_rows(named=True)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ chu·∫©n b·ªã pred_dict cho {len(pred_dict):,} users.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8495426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ƒêang t·∫°o hist_lookup t·ª´ d·ªØ li·ªáu l·ªãch s·ª≠...\n",
      "‚úÖ ƒê√£ chu·∫©n b·ªã xong l·ªãch s·ª≠ cho 628,069 kh√°ch h√†ng.\n"
     ]
    }
   ],
   "source": [
    "# --- T·∫†O HIST_LOOKUP (S·ª¨A L·ªñI .COLLECT) ---\n",
    "print(\"üîç ƒêang t·∫°o hist_lookup t·ª´ d·ªØ li·ªáu l·ªãch s·ª≠...\")\n",
    "\n",
    "# Ki·ªÉm tra n·∫øu user_items l√† LazyFrame th√¨ m·ªõi d√πng .collect()\n",
    "if isinstance(user_items, pl.LazyFrame):\n",
    "    user_items_df = user_items.select([\"customer_id\", \"item_id_list\"]).collect()\n",
    "else:\n",
    "    user_items_df = user_items.select([\"customer_id\", \"item_id_list\"])\n",
    "\n",
    "# T·∫°o Dictionary ƒë·ªÉ tra c·ª©u\n",
    "hist_lookup = {\n",
    "    str(row[\"customer_id\"]): set(map(str, row[\"item_id_list\"]))\n",
    "    for row in user_items_df.iter_rows(named=True)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ chu·∫©n b·ªã xong l·ªãch s·ª≠ cho {len(hist_lookup):,} kh√°ch h√†ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "203840be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ CH·∫æ ƒê·ªò ƒê√ÅNH GI√Å: B·∫¨T (T√≠nh c·∫£ 47k user mua ƒë·ªì c≈©)\n",
      "============================================================\n",
      "üî• NH√ìM WARM      : 0 users - P@10: nan\n",
      "‚ö° NH√ìM SEMI-COLD : 244,990 users - P@10: 0.005122\n",
      "‚ùÑÔ∏è NH√ìM PURE-COLD : 146,910 users - P@10: 0.005982\n",
      "------------------------------------------------------------\n",
      "üèÜ GLOBAL MEAN P@10 : 0.005444\n",
      "üì¶ T·ªïng User ƒë∆∞·ª£c t√≠nh: 391,900 / 391,900\n",
      "‚ÑπÔ∏è Trong ƒë√≥ c√≥ 47,470 users l√† nh√≥m 'Ch·ªâ mua l·∫°i ƒë·ªì c≈©'.\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/genyonguyen/.local/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/genyonguyen/.local/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_flexible(pred_dict, gt_dict, hist_lookup, warm_users_trained, K=10, include_repurchase=True):\n",
    "    warm_precs = []\n",
    "    semi_precs = []\n",
    "    pure_precs = []\n",
    "    \n",
    "    # 47k users n√†y th∆∞·ªùng thu·ªôc nh√≥m Warm/Semi-cold v√¨ h·ªç c√≥ l·ªãch s·ª≠\n",
    "    repurchase_only_count = 0 \n",
    "\n",
    "    for user_id, gt_items in gt_dict.items():\n",
    "        u_str = str(user_id)\n",
    "        current_gt = set(map(str, gt_items))\n",
    "        past_items = hist_lookup.get(u_str, set())\n",
    "        \n",
    "        # X√°c ƒë·ªãnh t·∫≠p ƒë·ªì m·ªõi\n",
    "        relevant_new_items = current_gt - past_items\n",
    "        \n",
    "        # LOGIC SI√äU THAM S·ªê:\n",
    "        if not relevant_new_items:\n",
    "            if not include_repurchase:\n",
    "                # T·∫Øt: B·ªè qua 47k users ch·ªâ mua ƒë·ªì c≈©\n",
    "                continue \n",
    "            else:\n",
    "                # B·∫≠t: Coi ƒë·ªì c≈© kh√°ch mua l·∫°i l√† m·ª•c ti√™u ƒë√°nh gi√° (Ground Truth)\n",
    "                target_items = current_gt\n",
    "                repurchase_only_count += 1\n",
    "        else:\n",
    "            # Lu√¥n t√≠nh nh·ªØng ng∆∞·ªùi c√≥ mua ƒë·ªì m·ªõi\n",
    "            target_items = relevant_new_items\n",
    "\n",
    "        # T√≠nh Precision@K\n",
    "        user_preds = pred_dict.get(u_str, [])[:K]\n",
    "        if not user_preds:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            hits = len(set(user_preds) & target_items)\n",
    "            precision = hits / K\n",
    "        \n",
    "        # Ph√¢n lu·ªìng b√°o c√°o d·ª±a tr√™n tr·∫°ng th√°i User\n",
    "        if u_str in warm_users_trained:\n",
    "            warm_precs.append(precision)\n",
    "        elif u_str in hist_lookup:\n",
    "            semi_precs.append(precision)\n",
    "        else:\n",
    "            pure_precs.append(precision)\n",
    "            \n",
    "    return warm_precs, semi_precs, pure_precs, repurchase_only_count\n",
    "\n",
    "# --- CH·∫†Y TH·ª¨ NGHI·ªÜM ---\n",
    "# B·∫°n c√≥ th·ªÉ thay ƒë·ªïi include_repurchase = True ho·∫∑c False ·ªü ƒë√¢y\n",
    "include_47k = True \n",
    "\n",
    "warm_p, semi_p, pure_p, re_count = evaluate_flexible(\n",
    "    pred_dict=pred_dict, \n",
    "    gt_dict=gt_dict, \n",
    "    hist_lookup=hist_lookup,\n",
    "    warm_users_trained=warm_users_set,\n",
    "    K=10,\n",
    "    include_repurchase=include_47k\n",
    ")\n",
    "\n",
    "# --- IN B√ÅO C√ÅO ---\n",
    "status = \"B·∫¨T (T√≠nh c·∫£ 47k user mua ƒë·ªì c≈©)\" if include_47k else \"T·∫ÆT (Ch·ªâ t√≠nh ƒë·ªì m·ªõi)\"\n",
    "print(f\"\\nüöÄ CH·∫æ ƒê·ªò ƒê√ÅNH GI√Å: {status}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üî• NH√ìM WARM      : {len(warm_p):,} users - P@10: {np.mean(warm_p):.6f}\")\n",
    "print(f\"‚ö° NH√ìM SEMI-COLD : {len(semi_p):,} users - P@10: {np.mean(semi_p):.6f}\")\n",
    "print(f\"‚ùÑÔ∏è NH√ìM PURE-COLD : {len(pure_p):,} users - P@10: {np.mean(pure_p):.6f}\")\n",
    "print(\"-\" * 60)\n",
    "total_evaluated = len(warm_p) + len(semi_p) + len(pure_p)\n",
    "print(f\"üèÜ GLOBAL MEAN P@10 : {np.mean(warm_p + semi_p + pure_p):.6f}\")\n",
    "print(f\"üì¶ T·ªïng User ƒë∆∞·ª£c t√≠nh: {total_evaluated:,} / {len(gt_dict):,}\")\n",
    "if include_47k:\n",
    "    print(f\"‚ÑπÔ∏è Trong ƒë√≥ c√≥ {re_count:,} users l√† nh√≥m 'Ch·ªâ mua l·∫°i ƒë·ªì c≈©'.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6ada9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
