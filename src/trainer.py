import xgboost as xgb
import joblib
import polars as pl

def train_model(train_df, feature_cols):
    print("â³ Collecting and processing training data...")
    
    # 1. Chuyá»ƒn Ä‘á»•i an toÃ n: 
    # Ã‰p kiá»ƒu features vá» Float32 vÃ  Ä‘áº£m báº£o nhÃ£n Y lÃ  sá»‘ nguyÃªn cho XGBoost
    train_pd = (
        train_df
        .select(feature_cols + ["Y"]) # Chá»‰ láº¥y cÃ¡c cá»™t cáº§n thiáº¿t Ä‘á»ƒ tiáº¿t kiá»‡m RAM
        .with_columns([
            pl.col(feature_cols).cast(pl.Float32).fill_null(0),
            pl.col("Y").cast(pl.Int8) # NhÃ£n 0/1 chá»‰ cáº§n Int8
        ])
        .collect()
        .to_pandas()
    )

    X_train = train_pd[feature_cols]
    y_train = train_pd["Y"]

    # 2. Khá»Ÿi táº¡o DMatrix (Ä‘á»‹nh dáº¡ng tá»‘i Æ°u cá»§a XGBoost)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    
    # 3. Cáº¥u hÃ¬nh tham sá»‘
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'logloss',  
        'eta': 0.05,               # TÄƒng nháº¹ tá»‘c Ä‘á»™ há»c náº¿u báº¡n tháº¥y 0.03 quÃ¡ cháº­m cho 800 rounds
        'max_depth': 8,            # 10 hÆ¡i sÃ¢u dá»… bá»‹ Overfit, 8 lÃ  Ä‘iá»ƒm cÃ¢n báº±ng tá»‘t cho Precision
        'subsample': 0.8,          
        'colsample_bytree': 0.8,   
        'tree_method': 'hist',     # Báº¯t buá»™c cho dá»¯ liá»‡u lá»›n (>1M dÃ²ng)
        'device': 'cpu',           # Äáº£m báº£o cháº¡y á»•n Ä‘á»‹nh trÃªn CPU
        'nthread': -1              
    }
    
    print(f"ğŸš€ Training XGBoost with {X_train.shape[0]:,} samples...")
    # Huáº¥n luyá»‡n model
    model = xgb.train(params, dtrain, num_boost_round=500) # 500 rounds thÆ°á»ng lÃ  Ä‘á»§ cho 3 features
    return model